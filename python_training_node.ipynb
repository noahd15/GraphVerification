{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6975ae06-82d8-4b20-a5b7-bb4296451d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from CybORG import CybORG\n",
    "# from CybORG.Simulator.Scenarios.DroneSwarmScenarioGenerator import DroneSwarmScenarioGenerator\n",
    "# from CybORG.Agents.SimpleAgents.RedDroneWorm import RedDroneWormAgent\n",
    "# from CybORG.Agents.Wrappers import PettingZooParallelWrapper\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "# from ipaddress import IPv4Address\n",
    "import random\n",
    "import logging\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd8be1-62eb-4bbf-bb71-be059a394214",
   "metadata": {},
   "source": [
    "# Train GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fe629",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd179ed-bc79-4074-8b19-94bacd3a42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 14000\n",
      "Test dataset length: 6000\n"
     ]
    }
   ],
   "source": [
    "filename = './data/dataset_python_node'\n",
    "# testfile = '../Data/testmulti'\n",
    "\n",
    "\n",
    "\n",
    "combined_dataset = torch.load(f'{filename}.pt', weights_only=False)\n",
    "# testset = torch.load(f'{testfile}.pt', weights_only=False)\n",
    "\n",
    "\n",
    "\n",
    "# combined_dataset = dataset + testset\n",
    "# print(f\"Combined dataset length: {len(combined_dataset)}\")\n",
    "random.shuffle(combined_dataset)\n",
    "\n",
    "size = len(combined_dataset)\n",
    "train_size = int(0.7 * size)\n",
    "\n",
    "dataset = combined_dataset[:train_size]\n",
    "testset = combined_dataset[train_size:]\n",
    "print(f\"Train dataset length: {len(dataset)}\")\n",
    "print(f\"Test dataset length: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f73ac4-5548-4004-b36d-288e220c18a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "\n",
      "Data(edge_index=[2, 70], num_nodes=18, x=[18, 110], y=[18])\n",
      "=============================================================\n",
      "Number of nodes: 18\n",
      "Number of edges: 70\n",
      "Average node degree: 3.89\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    delattr(data, 'feature')\n",
    "\n",
    "\n",
    "print(len(dataset))\n",
    "data = dataset[10]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06fb4ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 14000\n",
      "Number of test graphs: 6000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# random.shuffle(dataset)\n",
    "# random.shuffle(testset)\n",
    "# train_dataset = dataset[:12000]\n",
    "# test_dataset = dataset[12000:]\n",
    "\n",
    "# size = len(dataset)\n",
    "# size = size - size//4\n",
    "\n",
    "train_dataset = dataset\n",
    "test_dataset = testset\n",
    "\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3273c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if data.size(1) != 18:\n",
    "        print(f\"Warning: Data shape {data.shape} is not 18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "733f7409-47ab-4dd8-abc5-bf58ba570fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in train dataset: 110\n",
      "Number of classes detected: 3\n",
      "--------------Train--------------\n",
      "\u001b[1mTotal nodes: \u001b[0m252000\n",
      "\u001b[1mPercent of Not Compromised: \u001b[0m0.3497 (88116 nodes)\n",
      "\u001b[1mPercent of Compromised: \u001b[0m0.0156 (3922 nodes)\n",
      "\u001b[1mPercent of Highly Compromised: \u001b[0m0.6348 (159962 nodes)\n",
      "\n",
      "--------------Test--------------\n",
      "\u001b[1mTotal nodes: \u001b[0m108000\n",
      "\u001b[1mPercent of Not Compromised: \u001b[0m0.3565 (38501 nodes)\n",
      "\u001b[1mPercent of Compromised: \u001b[0m0.0152 (1643 nodes)\n",
      "\u001b[1mPercent of Highly Compromised: \u001b[0m0.6283 (67856 nodes)\n",
      "\n",
      "Class distribution:\n",
      "Not Compromised: █████████████████ 34.97%\n",
      "Compromised:  1.56%\n",
      "Highly Compromised: ███████████████████████████████ 63.48%\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = torch.load(f\"test_gl_selected.pt\", weights_only=False)\n",
    "# train_dataset = torch.load(f\"train_gl_selected.pt\", weights_only=False)\n",
    "\n",
    "test_dataset = testset\n",
    "train_dataset = dataset\n",
    "\n",
    "print(f'Number of features in train dataset: {train_dataset[0].x.shape[1]}')\n",
    "\n",
    "# Determine the number of classes in the dataset\n",
    "unique_classes = set()\n",
    "for t in train_dataset:\n",
    "    unique_classes.update(t.y.unique().cpu().numpy())\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"Number of classes detected: {num_classes}\")\n",
    "\n",
    "# Test dataset statistics\n",
    "test_total_nodes = 0\n",
    "test_class_counts = {i: 0 for i in range(num_classes)}\n",
    "\n",
    "for t in test_dataset:\n",
    "    test_total_nodes += t.y.size(0)\n",
    "    for class_idx in range(num_classes):\n",
    "        test_class_counts[class_idx] += (t.y == class_idx).sum().item()\n",
    "\n",
    "# Train dataset statistics\n",
    "train_total_nodes = 0\n",
    "train_class_counts = {i: 0 for i in range(num_classes)}\n",
    "\n",
    "for t in train_dataset:\n",
    "    train_total_nodes += t.y.size(0)\n",
    "    for class_idx in range(num_classes):\n",
    "        train_class_counts[class_idx] += (t.y == class_idx).sum().item()\n",
    "\n",
    "# Print statistics with class labels\n",
    "class_labels = [\"Not Compromised\", \"Compromised\", \"Highly Compromised\"] if num_classes == 3 else [f\"Class {i}\" for i in range(num_classes)]\n",
    "\n",
    "print('--------------Train--------------')\n",
    "print(f'\\033[1mTotal nodes: \\033[0m{train_total_nodes}')\n",
    "for i in range(num_classes):\n",
    "    percentage = train_class_counts[i]/train_total_nodes if train_total_nodes > 0 else 0\n",
    "    print(f'\\033[1mPercent of {class_labels[i]}: \\033[0m{percentage:.4f} ({train_class_counts[i]} nodes)')\n",
    "\n",
    "print('\\n--------------Test--------------')\n",
    "print(f'\\033[1mTotal nodes: \\033[0m{test_total_nodes}')\n",
    "for i in range(num_classes):\n",
    "    percentage = test_class_counts[i]/test_total_nodes if test_total_nodes > 0 else 0\n",
    "    print(f'\\033[1mPercent of {class_labels[i]}: \\033[0m{percentage:.4f} ({test_class_counts[i]} nodes)')\n",
    "\n",
    "# Print class distribution visualization if desired\n",
    "print(\"\\nClass distribution:\")\n",
    "max_bar_length = 50  # maximum bar length in characters\n",
    "for i in range(num_classes):\n",
    "    train_pct = train_class_counts[i]/train_total_nodes if train_total_nodes > 0 else 0\n",
    "    bar_length = int(train_pct * max_bar_length)\n",
    "    print(f\"{class_labels[i]}: {'█' * bar_length} {train_pct:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca1f5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "282f09a1-471b-4af4-b740-62a1642ea5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "GCN(\n",
      "  (conv1): GCNConv(110, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (conv3): GCNConv(32, 32)\n",
      "  (lin): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Specify device manually\n",
    "use_gpu = True # Set to False to use CPU\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# DataLoader (ensure `train_dataset` and `test_dataset` are loaded correctly)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "input_dim = len(train_dataset[0].x[0])\n",
    "\n",
    "# Define the GCN model for node classification\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim=input_dim,hidden_channels=32,num_classes=3):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(input_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)  # 2 classes: compromised or not\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # 2. Node-level classification\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b50f932-1389-42a8-af60-38bdc410cd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.12490650515809021, 2.806288018488087, 0.06880547635382327]\n",
      "Epoch: 001, Train Loss: 2.5258, Test Loss: 0.8941, Train Acc: 0.4577, Test Acc: 0.6804, Precision: 0.5503, Recall: 0.5876\n",
      "Epoch: 002, Train Loss: 1.2388, Test Loss: 0.8591, Train Acc: 0.5330, Test Acc: 0.6329, Precision: 0.5689, Recall: 0.5980\n",
      "Epoch: 003, Train Loss: 1.0474, Test Loss: 0.8512, Train Acc: 0.5559, Test Acc: 0.6467, Precision: 0.5690, Recall: 0.6042\n",
      "Epoch: 004, Train Loss: 0.9687, Test Loss: 0.8405, Train Acc: 0.5797, Test Acc: 0.6330, Precision: 0.5752, Recall: 0.6170\n",
      "Epoch: 005, Train Loss: 0.9391, Test Loss: 0.8334, Train Acc: 0.5886, Test Acc: 0.6147, Precision: 0.5835, Recall: 0.6235\n",
      "Epoch: 006, Train Loss: 0.9127, Test Loss: 0.8231, Train Acc: 0.5866, Test Acc: 0.6291, Precision: 0.5798, Recall: 0.6267\n",
      "Epoch: 007, Train Loss: 0.8969, Test Loss: 0.8163, Train Acc: 0.5926, Test Acc: 0.6127, Precision: 0.5864, Recall: 0.6370\n",
      "Epoch: 008, Train Loss: 0.8832, Test Loss: 0.8145, Train Acc: 0.5971, Test Acc: 0.6076, Precision: 0.5893, Recall: 0.6345\n",
      "Epoch: 009, Train Loss: 0.8734, Test Loss: 0.8076, Train Acc: 0.5991, Test Acc: 0.6043, Precision: 0.5907, Recall: 0.6372\n",
      "Epoch: 010, Train Loss: 0.8627, Test Loss: 0.8038, Train Acc: 0.5985, Test Acc: 0.6095, Precision: 0.5911, Recall: 0.6392\n",
      "Epoch: 011, Train Loss: 0.8513, Test Loss: 0.7976, Train Acc: 0.6043, Test Acc: 0.6151, Precision: 0.5877, Recall: 0.6356\n",
      "Epoch: 012, Train Loss: 0.8473, Test Loss: 0.7906, Train Acc: 0.6037, Test Acc: 0.6146, Precision: 0.5882, Recall: 0.6392\n",
      "Epoch: 013, Train Loss: 0.8406, Test Loss: 0.7894, Train Acc: 0.6048, Test Acc: 0.6179, Precision: 0.5893, Recall: 0.6406\n",
      "Epoch: 014, Train Loss: 0.8354, Test Loss: 0.7835, Train Acc: 0.6067, Test Acc: 0.6179, Precision: 0.5873, Recall: 0.6391\n",
      "Epoch: 015, Train Loss: 0.8292, Test Loss: 0.7801, Train Acc: 0.6088, Test Acc: 0.6112, Precision: 0.5899, Recall: 0.6382\n",
      "Epoch: 016, Train Loss: 0.8241, Test Loss: 0.7784, Train Acc: 0.6087, Test Acc: 0.6146, Precision: 0.5913, Recall: 0.6436\n",
      "Epoch: 017, Train Loss: 0.8208, Test Loss: 0.7737, Train Acc: 0.6134, Test Acc: 0.6208, Precision: 0.5894, Recall: 0.6434\n",
      "Epoch: 018, Train Loss: 0.8136, Test Loss: 0.7688, Train Acc: 0.6159, Test Acc: 0.6401, Precision: 0.5856, Recall: 0.6505\n",
      "Epoch: 019, Train Loss: 0.8087, Test Loss: 0.7643, Train Acc: 0.6226, Test Acc: 0.6435, Precision: 0.5834, Recall: 0.6491\n",
      "Epoch: 020, Train Loss: 0.8072, Test Loss: 0.7613, Train Acc: 0.6243, Test Acc: 0.6379, Precision: 0.5855, Recall: 0.6493\n",
      "Epoch: 021, Train Loss: 0.8015, Test Loss: 0.7584, Train Acc: 0.6257, Test Acc: 0.6495, Precision: 0.5828, Recall: 0.6526\n",
      "Epoch: 022, Train Loss: 0.7976, Test Loss: 0.7567, Train Acc: 0.6288, Test Acc: 0.6512, Precision: 0.5826, Recall: 0.6529\n",
      "Epoch: 023, Train Loss: 0.7943, Test Loss: 0.7521, Train Acc: 0.6276, Test Acc: 0.6455, Precision: 0.5848, Recall: 0.6530\n",
      "Epoch: 024, Train Loss: 0.7907, Test Loss: 0.7485, Train Acc: 0.6341, Test Acc: 0.6568, Precision: 0.5820, Recall: 0.6576\n",
      "Epoch: 025, Train Loss: 0.7886, Test Loss: 0.7448, Train Acc: 0.6321, Test Acc: 0.6677, Precision: 0.5796, Recall: 0.6610\n",
      "Epoch: 026, Train Loss: 0.7856, Test Loss: 0.7426, Train Acc: 0.6389, Test Acc: 0.6570, Precision: 0.5813, Recall: 0.6592\n",
      "Epoch: 027, Train Loss: 0.7837, Test Loss: 0.7386, Train Acc: 0.6398, Test Acc: 0.6581, Precision: 0.5813, Recall: 0.6600\n",
      "Epoch: 028, Train Loss: 0.7772, Test Loss: 0.7357, Train Acc: 0.6415, Test Acc: 0.6614, Precision: 0.5812, Recall: 0.6645\n",
      "Epoch: 029, Train Loss: 0.7748, Test Loss: 0.7337, Train Acc: 0.6470, Test Acc: 0.6616, Precision: 0.5827, Recall: 0.6666\n",
      "Epoch: 030, Train Loss: 0.7694, Test Loss: 0.7292, Train Acc: 0.6502, Test Acc: 0.6651, Precision: 0.5809, Recall: 0.6694\n",
      "Epoch: 031, Train Loss: 0.7636, Test Loss: 0.7270, Train Acc: 0.6490, Test Acc: 0.6731, Precision: 0.5797, Recall: 0.6730\n",
      "Epoch: 032, Train Loss: 0.7642, Test Loss: 0.7230, Train Acc: 0.6512, Test Acc: 0.6674, Precision: 0.5805, Recall: 0.6749\n",
      "Epoch: 033, Train Loss: 0.7635, Test Loss: 0.7216, Train Acc: 0.6508, Test Acc: 0.6838, Precision: 0.5758, Recall: 0.6777\n",
      "Epoch: 034, Train Loss: 0.7579, Test Loss: 0.7174, Train Acc: 0.6556, Test Acc: 0.6798, Precision: 0.5782, Recall: 0.6800\n",
      "Epoch: 035, Train Loss: 0.7537, Test Loss: 0.7142, Train Acc: 0.6563, Test Acc: 0.6840, Precision: 0.5768, Recall: 0.6844\n",
      "Epoch: 036, Train Loss: 0.7537, Test Loss: 0.7123, Train Acc: 0.6577, Test Acc: 0.6753, Precision: 0.5777, Recall: 0.6846\n",
      "Epoch: 037, Train Loss: 0.7488, Test Loss: 0.7088, Train Acc: 0.6587, Test Acc: 0.6864, Precision: 0.5778, Recall: 0.6871\n",
      "Epoch: 038, Train Loss: 0.7451, Test Loss: 0.7056, Train Acc: 0.6645, Test Acc: 0.6846, Precision: 0.5783, Recall: 0.6893\n",
      "Epoch: 039, Train Loss: 0.7452, Test Loss: 0.7025, Train Acc: 0.6629, Test Acc: 0.6936, Precision: 0.5762, Recall: 0.6949\n",
      "Epoch: 040, Train Loss: 0.7421, Test Loss: 0.6996, Train Acc: 0.6631, Test Acc: 0.6972, Precision: 0.5749, Recall: 0.6979\n",
      "Epoch: 041, Train Loss: 0.7381, Test Loss: 0.6967, Train Acc: 0.6688, Test Acc: 0.6957, Precision: 0.5760, Recall: 0.6983\n",
      "Epoch: 042, Train Loss: 0.7355, Test Loss: 0.6935, Train Acc: 0.6674, Test Acc: 0.6940, Precision: 0.5767, Recall: 0.7013\n",
      "Epoch: 043, Train Loss: 0.7364, Test Loss: 0.6929, Train Acc: 0.6712, Test Acc: 0.7038, Precision: 0.5744, Recall: 0.7049\n",
      "Epoch: 044, Train Loss: 0.7325, Test Loss: 0.6892, Train Acc: 0.6727, Test Acc: 0.6986, Precision: 0.5761, Recall: 0.7035\n",
      "Epoch: 045, Train Loss: 0.7279, Test Loss: 0.6868, Train Acc: 0.6714, Test Acc: 0.6927, Precision: 0.5764, Recall: 0.7027\n",
      "Epoch: 046, Train Loss: 0.7221, Test Loss: 0.6840, Train Acc: 0.6725, Test Acc: 0.7041, Precision: 0.5741, Recall: 0.7075\n",
      "Epoch: 047, Train Loss: 0.7251, Test Loss: 0.6811, Train Acc: 0.6748, Test Acc: 0.6983, Precision: 0.5742, Recall: 0.7085\n",
      "Epoch: 048, Train Loss: 0.7205, Test Loss: 0.6780, Train Acc: 0.6754, Test Acc: 0.7044, Precision: 0.5744, Recall: 0.7097\n",
      "Epoch: 049, Train Loss: 0.7195, Test Loss: 0.6750, Train Acc: 0.6761, Test Acc: 0.7043, Precision: 0.5743, Recall: 0.7099\n",
      "Epoch: 050, Train Loss: 0.7161, Test Loss: 0.6740, Train Acc: 0.6776, Test Acc: 0.7076, Precision: 0.5738, Recall: 0.7133\n",
      "Epoch: 051, Train Loss: 0.7141, Test Loss: 0.6721, Train Acc: 0.6801, Test Acc: 0.6953, Precision: 0.5741, Recall: 0.7115\n",
      "Epoch: 052, Train Loss: 0.7113, Test Loss: 0.6707, Train Acc: 0.6811, Test Acc: 0.7073, Precision: 0.5725, Recall: 0.7141\n",
      "Epoch: 053, Train Loss: 0.7108, Test Loss: 0.6659, Train Acc: 0.6801, Test Acc: 0.7059, Precision: 0.5731, Recall: 0.7148\n",
      "Epoch: 054, Train Loss: 0.7072, Test Loss: 0.6657, Train Acc: 0.6829, Test Acc: 0.7064, Precision: 0.5727, Recall: 0.7160\n",
      "Epoch: 055, Train Loss: 0.7039, Test Loss: 0.6629, Train Acc: 0.6816, Test Acc: 0.7088, Precision: 0.5726, Recall: 0.7166\n",
      "Epoch: 056, Train Loss: 0.7073, Test Loss: 0.6627, Train Acc: 0.6851, Test Acc: 0.7073, Precision: 0.5723, Recall: 0.7162\n",
      "Epoch: 057, Train Loss: 0.7055, Test Loss: 0.6619, Train Acc: 0.6862, Test Acc: 0.7007, Precision: 0.5731, Recall: 0.7167\n",
      "Epoch: 058, Train Loss: 0.6983, Test Loss: 0.6606, Train Acc: 0.6847, Test Acc: 0.7061, Precision: 0.5718, Recall: 0.7153\n",
      "Epoch: 059, Train Loss: 0.6976, Test Loss: 0.6576, Train Acc: 0.6864, Test Acc: 0.7079, Precision: 0.5712, Recall: 0.7186\n",
      "Epoch: 060, Train Loss: 0.6966, Test Loss: 0.6571, Train Acc: 0.6879, Test Acc: 0.7234, Precision: 0.5716, Recall: 0.7238\n",
      "Epoch: 061, Train Loss: 0.6940, Test Loss: 0.6543, Train Acc: 0.6907, Test Acc: 0.7113, Precision: 0.5718, Recall: 0.7235\n",
      "Epoch: 062, Train Loss: 0.6935, Test Loss: 0.6517, Train Acc: 0.6921, Test Acc: 0.7138, Precision: 0.5717, Recall: 0.7228\n",
      "Epoch: 063, Train Loss: 0.6889, Test Loss: 0.6520, Train Acc: 0.6922, Test Acc: 0.7139, Precision: 0.5709, Recall: 0.7262\n",
      "Epoch: 064, Train Loss: 0.6903, Test Loss: 0.6499, Train Acc: 0.6923, Test Acc: 0.7131, Precision: 0.5721, Recall: 0.7243\n",
      "Epoch: 065, Train Loss: 0.6833, Test Loss: 0.6489, Train Acc: 0.6936, Test Acc: 0.7218, Precision: 0.5715, Recall: 0.7271\n",
      "Epoch: 066, Train Loss: 0.6851, Test Loss: 0.6465, Train Acc: 0.6950, Test Acc: 0.7231, Precision: 0.5716, Recall: 0.7282\n",
      "Epoch: 067, Train Loss: 0.6851, Test Loss: 0.6455, Train Acc: 0.6923, Test Acc: 0.7201, Precision: 0.5717, Recall: 0.7302\n",
      "Epoch: 068, Train Loss: 0.6848, Test Loss: 0.6444, Train Acc: 0.6951, Test Acc: 0.7223, Precision: 0.5715, Recall: 0.7300\n",
      "Epoch: 069, Train Loss: 0.6829, Test Loss: 0.6426, Train Acc: 0.6976, Test Acc: 0.7180, Precision: 0.5713, Recall: 0.7289\n",
      "Epoch: 070, Train Loss: 0.6804, Test Loss: 0.6415, Train Acc: 0.6965, Test Acc: 0.7237, Precision: 0.5729, Recall: 0.7292\n",
      "Epoch: 071, Train Loss: 0.6788, Test Loss: 0.6406, Train Acc: 0.6984, Test Acc: 0.7152, Precision: 0.5716, Recall: 0.7304\n",
      "Epoch: 072, Train Loss: 0.6736, Test Loss: 0.6382, Train Acc: 0.6990, Test Acc: 0.7198, Precision: 0.5709, Recall: 0.7334\n",
      "Epoch: 073, Train Loss: 0.6750, Test Loss: 0.6381, Train Acc: 0.6995, Test Acc: 0.7230, Precision: 0.5707, Recall: 0.7330\n",
      "Epoch: 074, Train Loss: 0.6736, Test Loss: 0.6367, Train Acc: 0.6997, Test Acc: 0.7244, Precision: 0.5716, Recall: 0.7313\n",
      "Epoch: 075, Train Loss: 0.6744, Test Loss: 0.6363, Train Acc: 0.7007, Test Acc: 0.7212, Precision: 0.5707, Recall: 0.7322\n",
      "Epoch: 076, Train Loss: 0.6695, Test Loss: 0.6345, Train Acc: 0.7015, Test Acc: 0.7244, Precision: 0.5722, Recall: 0.7337\n",
      "Epoch: 077, Train Loss: 0.6729, Test Loss: 0.6340, Train Acc: 0.7014, Test Acc: 0.7284, Precision: 0.5712, Recall: 0.7348\n",
      "Epoch: 078, Train Loss: 0.6683, Test Loss: 0.6332, Train Acc: 0.7023, Test Acc: 0.7223, Precision: 0.5717, Recall: 0.7340\n",
      "Epoch: 079, Train Loss: 0.6649, Test Loss: 0.6309, Train Acc: 0.7028, Test Acc: 0.7301, Precision: 0.5712, Recall: 0.7365\n",
      "Epoch: 080, Train Loss: 0.6668, Test Loss: 0.6307, Train Acc: 0.7047, Test Acc: 0.7323, Precision: 0.5721, Recall: 0.7358\n",
      "Epoch: 081, Train Loss: 0.6645, Test Loss: 0.6298, Train Acc: 0.7063, Test Acc: 0.7282, Precision: 0.5708, Recall: 0.7357\n",
      "Epoch: 082, Train Loss: 0.6609, Test Loss: 0.6293, Train Acc: 0.7066, Test Acc: 0.7278, Precision: 0.5716, Recall: 0.7361\n",
      "Epoch: 083, Train Loss: 0.6631, Test Loss: 0.6287, Train Acc: 0.7054, Test Acc: 0.7257, Precision: 0.5710, Recall: 0.7349\n",
      "Epoch: 084, Train Loss: 0.6646, Test Loss: 0.6264, Train Acc: 0.7052, Test Acc: 0.7231, Precision: 0.5711, Recall: 0.7358\n",
      "Epoch: 085, Train Loss: 0.6607, Test Loss: 0.6268, Train Acc: 0.7066, Test Acc: 0.7271, Precision: 0.5714, Recall: 0.7360\n",
      "Epoch: 086, Train Loss: 0.6592, Test Loss: 0.6245, Train Acc: 0.7076, Test Acc: 0.7287, Precision: 0.5708, Recall: 0.7378\n",
      "Epoch: 087, Train Loss: 0.6573, Test Loss: 0.6231, Train Acc: 0.7077, Test Acc: 0.7320, Precision: 0.5718, Recall: 0.7379\n",
      "Epoch: 088, Train Loss: 0.6593, Test Loss: 0.6231, Train Acc: 0.7091, Test Acc: 0.7296, Precision: 0.5709, Recall: 0.7378\n",
      "Epoch: 089, Train Loss: 0.6592, Test Loss: 0.6231, Train Acc: 0.7085, Test Acc: 0.7296, Precision: 0.5715, Recall: 0.7388\n",
      "Epoch: 090, Train Loss: 0.6562, Test Loss: 0.6227, Train Acc: 0.7105, Test Acc: 0.7255, Precision: 0.5703, Recall: 0.7365\n",
      "Epoch: 091, Train Loss: 0.6558, Test Loss: 0.6218, Train Acc: 0.7102, Test Acc: 0.7259, Precision: 0.5704, Recall: 0.7370\n",
      "Epoch: 092, Train Loss: 0.6541, Test Loss: 0.6206, Train Acc: 0.7102, Test Acc: 0.7262, Precision: 0.5711, Recall: 0.7376\n",
      "Epoch: 093, Train Loss: 0.6518, Test Loss: 0.6195, Train Acc: 0.7094, Test Acc: 0.7247, Precision: 0.5703, Recall: 0.7364\n",
      "Epoch: 094, Train Loss: 0.6501, Test Loss: 0.6180, Train Acc: 0.7123, Test Acc: 0.7306, Precision: 0.5707, Recall: 0.7386\n",
      "Epoch: 095, Train Loss: 0.6555, Test Loss: 0.6186, Train Acc: 0.7102, Test Acc: 0.7263, Precision: 0.5699, Recall: 0.7367\n",
      "Epoch: 096, Train Loss: 0.6495, Test Loss: 0.6169, Train Acc: 0.7127, Test Acc: 0.7270, Precision: 0.5702, Recall: 0.7387\n",
      "Epoch: 097, Train Loss: 0.6500, Test Loss: 0.6163, Train Acc: 0.7120, Test Acc: 0.7304, Precision: 0.5712, Recall: 0.7395\n",
      "Epoch: 098, Train Loss: 0.6477, Test Loss: 0.6147, Train Acc: 0.7136, Test Acc: 0.7327, Precision: 0.5707, Recall: 0.7393\n",
      "Epoch: 099, Train Loss: 0.6468, Test Loss: 0.6150, Train Acc: 0.7142, Test Acc: 0.7338, Precision: 0.5707, Recall: 0.7411\n",
      "Epoch: 100, Train Loss: 0.6493, Test Loss: 0.6132, Train Acc: 0.7122, Test Acc: 0.7324, Precision: 0.5708, Recall: 0.7406\n",
      "Epoch: 101, Train Loss: 0.6446, Test Loss: 0.6140, Train Acc: 0.7140, Test Acc: 0.7282, Precision: 0.5699, Recall: 0.7397\n",
      "Epoch: 102, Train Loss: 0.6482, Test Loss: 0.6129, Train Acc: 0.7143, Test Acc: 0.7297, Precision: 0.5707, Recall: 0.7404\n",
      "Epoch: 103, Train Loss: 0.6470, Test Loss: 0.6115, Train Acc: 0.7128, Test Acc: 0.7333, Precision: 0.5710, Recall: 0.7413\n",
      "Epoch: 104, Train Loss: 0.6410, Test Loss: 0.6096, Train Acc: 0.7160, Test Acc: 0.7370, Precision: 0.5723, Recall: 0.7444\n",
      "Epoch: 105, Train Loss: 0.6387, Test Loss: 0.6090, Train Acc: 0.7173, Test Acc: 0.7343, Precision: 0.5715, Recall: 0.7432\n",
      "Epoch: 106, Train Loss: 0.6410, Test Loss: 0.6096, Train Acc: 0.7163, Test Acc: 0.7271, Precision: 0.5703, Recall: 0.7398\n",
      "Epoch: 107, Train Loss: 0.6412, Test Loss: 0.6104, Train Acc: 0.7158, Test Acc: 0.7312, Precision: 0.5700, Recall: 0.7415\n",
      "Epoch: 108, Train Loss: 0.6404, Test Loss: 0.6078, Train Acc: 0.7166, Test Acc: 0.7367, Precision: 0.5717, Recall: 0.7434\n",
      "Epoch: 109, Train Loss: 0.6408, Test Loss: 0.6086, Train Acc: 0.7170, Test Acc: 0.7368, Precision: 0.5715, Recall: 0.7428\n",
      "Epoch: 110, Train Loss: 0.6387, Test Loss: 0.6072, Train Acc: 0.7181, Test Acc: 0.7358, Precision: 0.5708, Recall: 0.7440\n",
      "Epoch: 111, Train Loss: 0.6379, Test Loss: 0.6061, Train Acc: 0.7175, Test Acc: 0.7323, Precision: 0.5700, Recall: 0.7441\n",
      "Epoch: 112, Train Loss: 0.6361, Test Loss: 0.6050, Train Acc: 0.7185, Test Acc: 0.7359, Precision: 0.5714, Recall: 0.7447\n",
      "Epoch: 113, Train Loss: 0.6340, Test Loss: 0.6050, Train Acc: 0.7196, Test Acc: 0.7327, Precision: 0.5713, Recall: 0.7430\n",
      "Epoch: 114, Train Loss: 0.6356, Test Loss: 0.6065, Train Acc: 0.7189, Test Acc: 0.7333, Precision: 0.5706, Recall: 0.7440\n",
      "Epoch: 115, Train Loss: 0.6328, Test Loss: 0.6034, Train Acc: 0.7197, Test Acc: 0.7395, Precision: 0.5722, Recall: 0.7451\n",
      "Epoch: 116, Train Loss: 0.6316, Test Loss: 0.6042, Train Acc: 0.7203, Test Acc: 0.7394, Precision: 0.5717, Recall: 0.7449\n",
      "Epoch: 117, Train Loss: 0.6311, Test Loss: 0.6029, Train Acc: 0.7194, Test Acc: 0.7395, Precision: 0.5719, Recall: 0.7465\n",
      "Epoch: 118, Train Loss: 0.6329, Test Loss: 0.6006, Train Acc: 0.7215, Test Acc: 0.7371, Precision: 0.5712, Recall: 0.7449\n",
      "Epoch: 119, Train Loss: 0.6328, Test Loss: 0.6016, Train Acc: 0.7196, Test Acc: 0.7383, Precision: 0.5720, Recall: 0.7469\n",
      "Epoch: 120, Train Loss: 0.6311, Test Loss: 0.6001, Train Acc: 0.7203, Test Acc: 0.7375, Precision: 0.5712, Recall: 0.7481\n",
      "Epoch: 121, Train Loss: 0.6325, Test Loss: 0.6011, Train Acc: 0.7193, Test Acc: 0.7363, Precision: 0.5709, Recall: 0.7460\n",
      "Epoch: 122, Train Loss: 0.6282, Test Loss: 0.5997, Train Acc: 0.7208, Test Acc: 0.7435, Precision: 0.5729, Recall: 0.7476\n",
      "Epoch: 123, Train Loss: 0.6288, Test Loss: 0.5981, Train Acc: 0.7218, Test Acc: 0.7406, Precision: 0.5722, Recall: 0.7472\n",
      "Epoch: 124, Train Loss: 0.6258, Test Loss: 0.5997, Train Acc: 0.7233, Test Acc: 0.7406, Precision: 0.5727, Recall: 0.7473\n",
      "Epoch: 125, Train Loss: 0.6275, Test Loss: 0.5983, Train Acc: 0.7216, Test Acc: 0.7402, Precision: 0.5723, Recall: 0.7475\n",
      "Epoch: 126, Train Loss: 0.6271, Test Loss: 0.5984, Train Acc: 0.7216, Test Acc: 0.7361, Precision: 0.5711, Recall: 0.7476\n",
      "Epoch: 127, Train Loss: 0.6231, Test Loss: 0.5974, Train Acc: 0.7225, Test Acc: 0.7454, Precision: 0.5730, Recall: 0.7489\n",
      "Epoch: 128, Train Loss: 0.6262, Test Loss: 0.5966, Train Acc: 0.7234, Test Acc: 0.7349, Precision: 0.5708, Recall: 0.7475\n",
      "Epoch: 129, Train Loss: 0.6222, Test Loss: 0.5957, Train Acc: 0.7224, Test Acc: 0.7361, Precision: 0.5713, Recall: 0.7495\n",
      "Epoch: 130, Train Loss: 0.6229, Test Loss: 0.5949, Train Acc: 0.7250, Test Acc: 0.7389, Precision: 0.5714, Recall: 0.7486\n",
      "Epoch: 131, Train Loss: 0.6261, Test Loss: 0.5961, Train Acc: 0.7220, Test Acc: 0.7396, Precision: 0.5720, Recall: 0.7473\n",
      "Epoch: 132, Train Loss: 0.6251, Test Loss: 0.5947, Train Acc: 0.7242, Test Acc: 0.7364, Precision: 0.5711, Recall: 0.7492\n",
      "Epoch: 133, Train Loss: 0.6244, Test Loss: 0.5949, Train Acc: 0.7229, Test Acc: 0.7413, Precision: 0.5714, Recall: 0.7486\n",
      "Epoch: 134, Train Loss: 0.6203, Test Loss: 0.5921, Train Acc: 0.7241, Test Acc: 0.7402, Precision: 0.5721, Recall: 0.7501\n",
      "Epoch: 135, Train Loss: 0.6231, Test Loss: 0.5950, Train Acc: 0.7237, Test Acc: 0.7326, Precision: 0.5702, Recall: 0.7495\n",
      "Epoch: 136, Train Loss: 0.6221, Test Loss: 0.5931, Train Acc: 0.7214, Test Acc: 0.7429, Precision: 0.5730, Recall: 0.7505\n",
      "Epoch: 137, Train Loss: 0.6217, Test Loss: 0.5921, Train Acc: 0.7240, Test Acc: 0.7372, Precision: 0.5707, Recall: 0.7498\n",
      "Epoch: 138, Train Loss: 0.6205, Test Loss: 0.5928, Train Acc: 0.7254, Test Acc: 0.7370, Precision: 0.5711, Recall: 0.7509\n",
      "Epoch: 139, Train Loss: 0.6190, Test Loss: 0.5913, Train Acc: 0.7255, Test Acc: 0.7426, Precision: 0.5733, Recall: 0.7532\n",
      "Epoch: 140, Train Loss: 0.6221, Test Loss: 0.5897, Train Acc: 0.7263, Test Acc: 0.7425, Precision: 0.5731, Recall: 0.7527\n",
      "Epoch: 141, Train Loss: 0.6178, Test Loss: 0.5902, Train Acc: 0.7269, Test Acc: 0.7450, Precision: 0.5731, Recall: 0.7517\n",
      "Epoch: 142, Train Loss: 0.6155, Test Loss: 0.5898, Train Acc: 0.7273, Test Acc: 0.7379, Precision: 0.5715, Recall: 0.7535\n",
      "Epoch: 143, Train Loss: 0.6201, Test Loss: 0.5899, Train Acc: 0.7253, Test Acc: 0.7386, Precision: 0.5713, Recall: 0.7517\n",
      "Epoch: 144, Train Loss: 0.6163, Test Loss: 0.5886, Train Acc: 0.7250, Test Acc: 0.7434, Precision: 0.5732, Recall: 0.7538\n",
      "Epoch: 145, Train Loss: 0.6154, Test Loss: 0.5879, Train Acc: 0.7271, Test Acc: 0.7434, Precision: 0.5736, Recall: 0.7542\n",
      "Epoch: 146, Train Loss: 0.6220, Test Loss: 0.5880, Train Acc: 0.7250, Test Acc: 0.7416, Precision: 0.5724, Recall: 0.7546\n",
      "Epoch: 147, Train Loss: 0.6176, Test Loss: 0.5872, Train Acc: 0.7250, Test Acc: 0.7433, Precision: 0.5732, Recall: 0.7542\n",
      "Epoch: 148, Train Loss: 0.6134, Test Loss: 0.5874, Train Acc: 0.7267, Test Acc: 0.7427, Precision: 0.5725, Recall: 0.7537\n",
      "Epoch: 149, Train Loss: 0.6130, Test Loss: 0.5880, Train Acc: 0.7272, Test Acc: 0.7399, Precision: 0.5718, Recall: 0.7542\n",
      "Epoch: 150, Train Loss: 0.6158, Test Loss: 0.5873, Train Acc: 0.7261, Test Acc: 0.7426, Precision: 0.5731, Recall: 0.7544\n",
      "Epoch: 151, Train Loss: 0.6128, Test Loss: 0.5867, Train Acc: 0.7278, Test Acc: 0.7465, Precision: 0.5743, Recall: 0.7546\n",
      "Epoch: 152, Train Loss: 0.6099, Test Loss: 0.5870, Train Acc: 0.7278, Test Acc: 0.7404, Precision: 0.5730, Recall: 0.7547\n",
      "Epoch: 153, Train Loss: 0.6163, Test Loss: 0.5889, Train Acc: 0.7262, Test Acc: 0.7398, Precision: 0.5727, Recall: 0.7541\n",
      "Epoch: 154, Train Loss: 0.6102, Test Loss: 0.5869, Train Acc: 0.7281, Test Acc: 0.7441, Precision: 0.5735, Recall: 0.7559\n",
      "Epoch: 155, Train Loss: 0.6112, Test Loss: 0.5842, Train Acc: 0.7274, Test Acc: 0.7438, Precision: 0.5734, Recall: 0.7568\n",
      "Epoch: 156, Train Loss: 0.6127, Test Loss: 0.5841, Train Acc: 0.7276, Test Acc: 0.7441, Precision: 0.5739, Recall: 0.7561\n",
      "Epoch: 157, Train Loss: 0.6138, Test Loss: 0.5854, Train Acc: 0.7266, Test Acc: 0.7447, Precision: 0.5740, Recall: 0.7552\n",
      "Epoch: 158, Train Loss: 0.6108, Test Loss: 0.5843, Train Acc: 0.7271, Test Acc: 0.7451, Precision: 0.5736, Recall: 0.7552\n",
      "Epoch: 159, Train Loss: 0.6094, Test Loss: 0.5839, Train Acc: 0.7288, Test Acc: 0.7445, Precision: 0.5740, Recall: 0.7576\n",
      "Epoch: 160, Train Loss: 0.6092, Test Loss: 0.5830, Train Acc: 0.7287, Test Acc: 0.7465, Precision: 0.5741, Recall: 0.7569\n",
      "Epoch: 161, Train Loss: 0.6113, Test Loss: 0.5850, Train Acc: 0.7280, Test Acc: 0.7435, Precision: 0.5731, Recall: 0.7560\n",
      "Epoch: 162, Train Loss: 0.6094, Test Loss: 0.5813, Train Acc: 0.7303, Test Acc: 0.7464, Precision: 0.5746, Recall: 0.7574\n",
      "Epoch: 163, Train Loss: 0.6076, Test Loss: 0.5828, Train Acc: 0.7290, Test Acc: 0.7454, Precision: 0.5739, Recall: 0.7572\n",
      "Epoch: 164, Train Loss: 0.6080, Test Loss: 0.5844, Train Acc: 0.7287, Test Acc: 0.7440, Precision: 0.5732, Recall: 0.7563\n",
      "Epoch: 165, Train Loss: 0.6048, Test Loss: 0.5822, Train Acc: 0.7294, Test Acc: 0.7480, Precision: 0.5748, Recall: 0.7572\n",
      "Epoch: 166, Train Loss: 0.6097, Test Loss: 0.5843, Train Acc: 0.7285, Test Acc: 0.7407, Precision: 0.5723, Recall: 0.7560\n",
      "Epoch: 167, Train Loss: 0.6057, Test Loss: 0.5796, Train Acc: 0.7289, Test Acc: 0.7490, Precision: 0.5753, Recall: 0.7581\n",
      "Epoch: 168, Train Loss: 0.6051, Test Loss: 0.5821, Train Acc: 0.7283, Test Acc: 0.7471, Precision: 0.5745, Recall: 0.7572\n",
      "Epoch: 169, Train Loss: 0.6080, Test Loss: 0.5820, Train Acc: 0.7310, Test Acc: 0.7452, Precision: 0.5739, Recall: 0.7582\n",
      "Epoch: 170, Train Loss: 0.6077, Test Loss: 0.5795, Train Acc: 0.7298, Test Acc: 0.7469, Precision: 0.5746, Recall: 0.7580\n",
      "Epoch: 171, Train Loss: 0.6038, Test Loss: 0.5807, Train Acc: 0.7292, Test Acc: 0.7457, Precision: 0.5734, Recall: 0.7565\n",
      "Epoch: 172, Train Loss: 0.6046, Test Loss: 0.5784, Train Acc: 0.7309, Test Acc: 0.7466, Precision: 0.5743, Recall: 0.7586\n",
      "Epoch: 173, Train Loss: 0.6027, Test Loss: 0.5805, Train Acc: 0.7309, Test Acc: 0.7485, Precision: 0.5751, Recall: 0.7582\n",
      "Epoch: 174, Train Loss: 0.6045, Test Loss: 0.5787, Train Acc: 0.7308, Test Acc: 0.7481, Precision: 0.5750, Recall: 0.7585\n",
      "Epoch: 175, Train Loss: 0.6019, Test Loss: 0.5794, Train Acc: 0.7310, Test Acc: 0.7456, Precision: 0.5742, Recall: 0.7596\n",
      "Epoch: 176, Train Loss: 0.6002, Test Loss: 0.5786, Train Acc: 0.7305, Test Acc: 0.7495, Precision: 0.5757, Recall: 0.7600\n",
      "Epoch: 177, Train Loss: 0.5996, Test Loss: 0.5807, Train Acc: 0.7314, Test Acc: 0.7453, Precision: 0.5741, Recall: 0.7593\n",
      "Epoch: 178, Train Loss: 0.6019, Test Loss: 0.5786, Train Acc: 0.7306, Test Acc: 0.7469, Precision: 0.5747, Recall: 0.7594\n",
      "Epoch: 179, Train Loss: 0.6009, Test Loss: 0.5773, Train Acc: 0.7299, Test Acc: 0.7484, Precision: 0.5753, Recall: 0.7594\n",
      "Epoch: 180, Train Loss: 0.6001, Test Loss: 0.5793, Train Acc: 0.7303, Test Acc: 0.7459, Precision: 0.5741, Recall: 0.7583\n",
      "Epoch: 181, Train Loss: 0.6011, Test Loss: 0.5783, Train Acc: 0.7299, Test Acc: 0.7486, Precision: 0.5749, Recall: 0.7595\n",
      "Epoch: 182, Train Loss: 0.5997, Test Loss: 0.5788, Train Acc: 0.7303, Test Acc: 0.7496, Precision: 0.5757, Recall: 0.7613\n",
      "Epoch: 183, Train Loss: 0.6000, Test Loss: 0.5777, Train Acc: 0.7318, Test Acc: 0.7473, Precision: 0.5748, Recall: 0.7596\n",
      "Epoch: 184, Train Loss: 0.6020, Test Loss: 0.5770, Train Acc: 0.7313, Test Acc: 0.7450, Precision: 0.5739, Recall: 0.7600\n",
      "Epoch: 185, Train Loss: 0.5993, Test Loss: 0.5739, Train Acc: 0.7308, Test Acc: 0.7514, Precision: 0.5764, Recall: 0.7620\n",
      "Epoch: 186, Train Loss: 0.5985, Test Loss: 0.5775, Train Acc: 0.7308, Test Acc: 0.7446, Precision: 0.5741, Recall: 0.7593\n",
      "Epoch: 187, Train Loss: 0.5983, Test Loss: 0.5747, Train Acc: 0.7318, Test Acc: 0.7506, Precision: 0.5762, Recall: 0.7617\n",
      "Epoch: 188, Train Loss: 0.5993, Test Loss: 0.5751, Train Acc: 0.7323, Test Acc: 0.7506, Precision: 0.5762, Recall: 0.7615\n",
      "Epoch: 189, Train Loss: 0.5971, Test Loss: 0.5742, Train Acc: 0.7329, Test Acc: 0.7480, Precision: 0.5749, Recall: 0.7610\n",
      "Epoch: 190, Train Loss: 0.5957, Test Loss: 0.5742, Train Acc: 0.7311, Test Acc: 0.7483, Precision: 0.5754, Recall: 0.7612\n",
      "Epoch: 191, Train Loss: 0.5977, Test Loss: 0.5790, Train Acc: 0.7332, Test Acc: 0.7421, Precision: 0.5736, Recall: 0.7585\n",
      "Epoch: 192, Train Loss: 0.5986, Test Loss: 0.5751, Train Acc: 0.7305, Test Acc: 0.7479, Precision: 0.5749, Recall: 0.7605\n",
      "Epoch: 193, Train Loss: 0.5958, Test Loss: 0.5767, Train Acc: 0.7322, Test Acc: 0.7506, Precision: 0.5763, Recall: 0.7613\n",
      "Epoch: 194, Train Loss: 0.5930, Test Loss: 0.5781, Train Acc: 0.7314, Test Acc: 0.7444, Precision: 0.5736, Recall: 0.7599\n",
      "Epoch: 195, Train Loss: 0.5947, Test Loss: 0.5768, Train Acc: 0.7315, Test Acc: 0.7458, Precision: 0.5743, Recall: 0.7606\n",
      "Epoch: 196, Train Loss: 0.5954, Test Loss: 0.5745, Train Acc: 0.7326, Test Acc: 0.7488, Precision: 0.5757, Recall: 0.7625\n",
      "Epoch: 197, Train Loss: 0.5961, Test Loss: 0.5746, Train Acc: 0.7312, Test Acc: 0.7513, Precision: 0.5771, Recall: 0.7640\n",
      "Epoch: 198, Train Loss: 0.5945, Test Loss: 0.5734, Train Acc: 0.7324, Test Acc: 0.7512, Precision: 0.5766, Recall: 0.7621\n",
      "Epoch: 199, Train Loss: 0.5910, Test Loss: 0.5748, Train Acc: 0.7319, Test Acc: 0.7496, Precision: 0.5761, Recall: 0.7633\n",
      "Epoch: 200, Train Loss: 0.5921, Test Loss: 0.5749, Train Acc: 0.7310, Test Acc: 0.7500, Precision: 0.5766, Recall: 0.7635\n",
      "Epoch: 201, Train Loss: 0.5974, Test Loss: 0.5743, Train Acc: 0.7319, Test Acc: 0.7519, Precision: 0.5772, Recall: 0.7635\n",
      "Epoch: 202, Train Loss: 0.5922, Test Loss: 0.5760, Train Acc: 0.7320, Test Acc: 0.7465, Precision: 0.5747, Recall: 0.7602\n",
      "Epoch: 203, Train Loss: 0.5950, Test Loss: 0.5748, Train Acc: 0.7307, Test Acc: 0.7485, Precision: 0.5753, Recall: 0.7604\n",
      "Epoch: 204, Train Loss: 0.5966, Test Loss: 0.5756, Train Acc: 0.7314, Test Acc: 0.7477, Precision: 0.5754, Recall: 0.7624\n",
      "Epoch: 205, Train Loss: 0.5896, Test Loss: 0.5733, Train Acc: 0.7331, Test Acc: 0.7493, Precision: 0.5758, Recall: 0.7632\n",
      "Epoch: 206, Train Loss: 0.5893, Test Loss: 0.5712, Train Acc: 0.7332, Test Acc: 0.7519, Precision: 0.5770, Recall: 0.7633\n",
      "Epoch: 207, Train Loss: 0.5914, Test Loss: 0.5726, Train Acc: 0.7316, Test Acc: 0.7554, Precision: 0.5788, Recall: 0.7641\n",
      "Epoch: 208, Train Loss: 0.5895, Test Loss: 0.5718, Train Acc: 0.7336, Test Acc: 0.7499, Precision: 0.5763, Recall: 0.7634\n",
      "Epoch: 209, Train Loss: 0.5901, Test Loss: 0.5707, Train Acc: 0.7325, Test Acc: 0.7500, Precision: 0.5762, Recall: 0.7631\n",
      "Epoch: 210, Train Loss: 0.5925, Test Loss: 0.5711, Train Acc: 0.7314, Test Acc: 0.7490, Precision: 0.5759, Recall: 0.7641\n",
      "Epoch: 211, Train Loss: 0.5879, Test Loss: 0.5713, Train Acc: 0.7321, Test Acc: 0.7546, Precision: 0.5784, Recall: 0.7650\n",
      "Epoch: 212, Train Loss: 0.5928, Test Loss: 0.5704, Train Acc: 0.7322, Test Acc: 0.7506, Precision: 0.5763, Recall: 0.7634\n",
      "Epoch: 213, Train Loss: 0.5901, Test Loss: 0.5712, Train Acc: 0.7324, Test Acc: 0.7518, Precision: 0.5769, Recall: 0.7631\n",
      "Epoch: 214, Train Loss: 0.5865, Test Loss: 0.5724, Train Acc: 0.7340, Test Acc: 0.7499, Precision: 0.5762, Recall: 0.7637\n",
      "Epoch: 215, Train Loss: 0.5854, Test Loss: 0.5724, Train Acc: 0.7327, Test Acc: 0.7495, Precision: 0.5759, Recall: 0.7631\n",
      "Epoch: 216, Train Loss: 0.5889, Test Loss: 0.5714, Train Acc: 0.7332, Test Acc: 0.7457, Precision: 0.5745, Recall: 0.7636\n",
      "Epoch: 217, Train Loss: 0.5890, Test Loss: 0.5713, Train Acc: 0.7327, Test Acc: 0.7516, Precision: 0.5773, Recall: 0.7653\n",
      "Epoch: 218, Train Loss: 0.5893, Test Loss: 0.5717, Train Acc: 0.7330, Test Acc: 0.7496, Precision: 0.5764, Recall: 0.7633\n",
      "Epoch: 219, Train Loss: 0.5878, Test Loss: 0.5706, Train Acc: 0.7320, Test Acc: 0.7505, Precision: 0.5765, Recall: 0.7635\n",
      "Epoch: 220, Train Loss: 0.5868, Test Loss: 0.5701, Train Acc: 0.7337, Test Acc: 0.7537, Precision: 0.5782, Recall: 0.7645\n",
      "Epoch: 221, Train Loss: 0.5903, Test Loss: 0.5691, Train Acc: 0.7335, Test Acc: 0.7515, Precision: 0.5770, Recall: 0.7645\n",
      "Epoch: 222, Train Loss: 0.5870, Test Loss: 0.5693, Train Acc: 0.7334, Test Acc: 0.7523, Precision: 0.5779, Recall: 0.7655\n",
      "Epoch: 223, Train Loss: 0.5872, Test Loss: 0.5690, Train Acc: 0.7336, Test Acc: 0.7551, Precision: 0.5789, Recall: 0.7665\n",
      "Epoch: 224, Train Loss: 0.5874, Test Loss: 0.5708, Train Acc: 0.7322, Test Acc: 0.7518, Precision: 0.5771, Recall: 0.7633\n",
      "Epoch: 225, Train Loss: 0.5832, Test Loss: 0.5697, Train Acc: 0.7337, Test Acc: 0.7491, Precision: 0.5760, Recall: 0.7642\n",
      "Epoch: 226, Train Loss: 0.5866, Test Loss: 0.5682, Train Acc: 0.7346, Test Acc: 0.7529, Precision: 0.5778, Recall: 0.7650\n",
      "Epoch: 227, Train Loss: 0.5843, Test Loss: 0.5685, Train Acc: 0.7328, Test Acc: 0.7528, Precision: 0.5777, Recall: 0.7647\n",
      "Epoch: 228, Train Loss: 0.5864, Test Loss: 0.5678, Train Acc: 0.7336, Test Acc: 0.7559, Precision: 0.5791, Recall: 0.7652\n",
      "Epoch: 229, Train Loss: 0.5849, Test Loss: 0.5681, Train Acc: 0.7328, Test Acc: 0.7549, Precision: 0.5786, Recall: 0.7646\n",
      "Epoch: 230, Train Loss: 0.5865, Test Loss: 0.5674, Train Acc: 0.7344, Test Acc: 0.7523, Precision: 0.5775, Recall: 0.7650\n",
      "Epoch: 231, Train Loss: 0.5838, Test Loss: 0.5693, Train Acc: 0.7333, Test Acc: 0.7523, Precision: 0.5773, Recall: 0.7643\n",
      "Epoch: 232, Train Loss: 0.5842, Test Loss: 0.5683, Train Acc: 0.7339, Test Acc: 0.7516, Precision: 0.5773, Recall: 0.7662\n",
      "Epoch: 233, Train Loss: 0.5856, Test Loss: 0.5691, Train Acc: 0.7327, Test Acc: 0.7534, Precision: 0.5781, Recall: 0.7657\n",
      "Epoch: 234, Train Loss: 0.5836, Test Loss: 0.5675, Train Acc: 0.7340, Test Acc: 0.7559, Precision: 0.5796, Recall: 0.7671\n",
      "Epoch: 235, Train Loss: 0.5855, Test Loss: 0.5684, Train Acc: 0.7352, Test Acc: 0.7527, Precision: 0.5776, Recall: 0.7642\n",
      "Epoch: 236, Train Loss: 0.5833, Test Loss: 0.5671, Train Acc: 0.7333, Test Acc: 0.7538, Precision: 0.5782, Recall: 0.7653\n",
      "Epoch: 237, Train Loss: 0.5846, Test Loss: 0.5709, Train Acc: 0.7326, Test Acc: 0.7517, Precision: 0.5772, Recall: 0.7640\n",
      "Epoch: 238, Train Loss: 0.5825, Test Loss: 0.5703, Train Acc: 0.7331, Test Acc: 0.7528, Precision: 0.5782, Recall: 0.7670\n",
      "Epoch: 239, Train Loss: 0.5801, Test Loss: 0.5692, Train Acc: 0.7339, Test Acc: 0.7540, Precision: 0.5785, Recall: 0.7661\n",
      "Epoch: 240, Train Loss: 0.5840, Test Loss: 0.5678, Train Acc: 0.7318, Test Acc: 0.7523, Precision: 0.5776, Recall: 0.7652\n",
      "Epoch: 241, Train Loss: 0.5823, Test Loss: 0.5676, Train Acc: 0.7341, Test Acc: 0.7552, Precision: 0.5789, Recall: 0.7658\n",
      "Epoch: 242, Train Loss: 0.5806, Test Loss: 0.5672, Train Acc: 0.7341, Test Acc: 0.7548, Precision: 0.5787, Recall: 0.7657\n",
      "Epoch: 243, Train Loss: 0.5820, Test Loss: 0.5655, Train Acc: 0.7341, Test Acc: 0.7557, Precision: 0.5793, Recall: 0.7670\n",
      "Epoch: 244, Train Loss: 0.5802, Test Loss: 0.5683, Train Acc: 0.7336, Test Acc: 0.7575, Precision: 0.5804, Recall: 0.7673\n",
      "Epoch: 245, Train Loss: 0.5792, Test Loss: 0.5688, Train Acc: 0.7348, Test Acc: 0.7502, Precision: 0.5766, Recall: 0.7656\n",
      "Epoch: 246, Train Loss: 0.5807, Test Loss: 0.5668, Train Acc: 0.7330, Test Acc: 0.7508, Precision: 0.5771, Recall: 0.7669\n",
      "Epoch: 247, Train Loss: 0.5803, Test Loss: 0.5677, Train Acc: 0.7333, Test Acc: 0.7519, Precision: 0.5777, Recall: 0.7670\n",
      "Epoch: 248, Train Loss: 0.5813, Test Loss: 0.5691, Train Acc: 0.7334, Test Acc: 0.7471, Precision: 0.5752, Recall: 0.7637\n",
      "Epoch: 249, Train Loss: 0.5830, Test Loss: 0.5659, Train Acc: 0.7333, Test Acc: 0.7520, Precision: 0.5774, Recall: 0.7659\n",
      "Epoch: 250, Train Loss: 0.5799, Test Loss: 0.5675, Train Acc: 0.7338, Test Acc: 0.7528, Precision: 0.5780, Recall: 0.7671\n",
      "Epoch: 251, Train Loss: 0.5796, Test Loss: 0.5644, Train Acc: 0.7347, Test Acc: 0.7558, Precision: 0.5792, Recall: 0.7660\n",
      "Epoch: 252, Train Loss: 0.5803, Test Loss: 0.5680, Train Acc: 0.7347, Test Acc: 0.7512, Precision: 0.5775, Recall: 0.7661\n",
      "Epoch: 253, Train Loss: 0.5775, Test Loss: 0.5666, Train Acc: 0.7334, Test Acc: 0.7546, Precision: 0.5788, Recall: 0.7662\n",
      "Epoch: 254, Train Loss: 0.5784, Test Loss: 0.5683, Train Acc: 0.7342, Test Acc: 0.7542, Precision: 0.5787, Recall: 0.7670\n",
      "Epoch: 255, Train Loss: 0.5773, Test Loss: 0.5662, Train Acc: 0.7346, Test Acc: 0.7550, Precision: 0.5790, Recall: 0.7670\n",
      "Epoch: 256, Train Loss: 0.5804, Test Loss: 0.5667, Train Acc: 0.7346, Test Acc: 0.7533, Precision: 0.5779, Recall: 0.7655\n",
      "Epoch: 257, Train Loss: 0.5757, Test Loss: 0.5661, Train Acc: 0.7333, Test Acc: 0.7587, Precision: 0.5814, Recall: 0.7695\n",
      "Epoch: 258, Train Loss: 0.5802, Test Loss: 0.5662, Train Acc: 0.7343, Test Acc: 0.7535, Precision: 0.5781, Recall: 0.7660\n",
      "Epoch: 259, Train Loss: 0.5817, Test Loss: 0.5661, Train Acc: 0.7338, Test Acc: 0.7511, Precision: 0.5771, Recall: 0.7663\n",
      "Epoch: 260, Train Loss: 0.5781, Test Loss: 0.5679, Train Acc: 0.7353, Test Acc: 0.7539, Precision: 0.5788, Recall: 0.7667\n",
      "Epoch: 261, Train Loss: 0.5753, Test Loss: 0.5657, Train Acc: 0.7349, Test Acc: 0.7548, Precision: 0.5790, Recall: 0.7672\n",
      "Epoch: 262, Train Loss: 0.5760, Test Loss: 0.5682, Train Acc: 0.7335, Test Acc: 0.7545, Precision: 0.5790, Recall: 0.7666\n",
      "Epoch: 263, Train Loss: 0.5769, Test Loss: 0.5666, Train Acc: 0.7349, Test Acc: 0.7557, Precision: 0.5795, Recall: 0.7669\n",
      "Epoch: 264, Train Loss: 0.5754, Test Loss: 0.5650, Train Acc: 0.7343, Test Acc: 0.7544, Precision: 0.5790, Recall: 0.7667\n",
      "Epoch: 265, Train Loss: 0.5795, Test Loss: 0.5657, Train Acc: 0.7346, Test Acc: 0.7526, Precision: 0.5778, Recall: 0.7662\n",
      "Epoch: 266, Train Loss: 0.5805, Test Loss: 0.5659, Train Acc: 0.7340, Test Acc: 0.7521, Precision: 0.5776, Recall: 0.7655\n",
      "Epoch: 267, Train Loss: 0.5753, Test Loss: 0.5674, Train Acc: 0.7345, Test Acc: 0.7533, Precision: 0.5784, Recall: 0.7668\n",
      "Epoch: 268, Train Loss: 0.5786, Test Loss: 0.5654, Train Acc: 0.7334, Test Acc: 0.7530, Precision: 0.5781, Recall: 0.7672\n",
      "Epoch: 269, Train Loss: 0.5753, Test Loss: 0.5656, Train Acc: 0.7329, Test Acc: 0.7581, Precision: 0.5808, Recall: 0.7673\n",
      "Epoch: 270, Train Loss: 0.5772, Test Loss: 0.5664, Train Acc: 0.7344, Test Acc: 0.7566, Precision: 0.5798, Recall: 0.7661\n",
      "Epoch: 271, Train Loss: 0.5721, Test Loss: 0.5668, Train Acc: 0.7342, Test Acc: 0.7533, Precision: 0.5782, Recall: 0.7665\n",
      "Epoch: 272, Train Loss: 0.5767, Test Loss: 0.5655, Train Acc: 0.7329, Test Acc: 0.7559, Precision: 0.5794, Recall: 0.7671\n",
      "Epoch: 273, Train Loss: 0.5764, Test Loss: 0.5655, Train Acc: 0.7356, Test Acc: 0.7525, Precision: 0.5781, Recall: 0.7663\n",
      "Epoch: 274, Train Loss: 0.5735, Test Loss: 0.5662, Train Acc: 0.7354, Test Acc: 0.7566, Precision: 0.5801, Recall: 0.7674\n",
      "Epoch: 275, Train Loss: 0.5749, Test Loss: 0.5652, Train Acc: 0.7347, Test Acc: 0.7532, Precision: 0.5783, Recall: 0.7660\n",
      "Epoch: 276, Train Loss: 0.5711, Test Loss: 0.5633, Train Acc: 0.7351, Test Acc: 0.7559, Precision: 0.5795, Recall: 0.7669\n",
      "Epoch: 277, Train Loss: 0.5710, Test Loss: 0.5654, Train Acc: 0.7357, Test Acc: 0.7595, Precision: 0.5815, Recall: 0.7674\n",
      "Epoch: 278, Train Loss: 0.5710, Test Loss: 0.5673, Train Acc: 0.7355, Test Acc: 0.7596, Precision: 0.5815, Recall: 0.7681\n",
      "Epoch: 279, Train Loss: 0.5720, Test Loss: 0.5668, Train Acc: 0.7358, Test Acc: 0.7560, Precision: 0.5798, Recall: 0.7681\n",
      "Epoch: 280, Train Loss: 0.5757, Test Loss: 0.5670, Train Acc: 0.7343, Test Acc: 0.7532, Precision: 0.5784, Recall: 0.7663\n",
      "Epoch: 281, Train Loss: 0.5722, Test Loss: 0.5653, Train Acc: 0.7349, Test Acc: 0.7548, Precision: 0.5790, Recall: 0.7667\n",
      "Epoch: 282, Train Loss: 0.5732, Test Loss: 0.5634, Train Acc: 0.7351, Test Acc: 0.7580, Precision: 0.5808, Recall: 0.7688\n",
      "Epoch: 283, Train Loss: 0.5722, Test Loss: 0.5655, Train Acc: 0.7341, Test Acc: 0.7582, Precision: 0.5809, Recall: 0.7684\n",
      "Epoch: 284, Train Loss: 0.5690, Test Loss: 0.5648, Train Acc: 0.7345, Test Acc: 0.7591, Precision: 0.5814, Recall: 0.7683\n",
      "Epoch: 285, Train Loss: 0.5677, Test Loss: 0.5654, Train Acc: 0.7359, Test Acc: 0.7561, Precision: 0.5798, Recall: 0.7681\n",
      "Epoch: 286, Train Loss: 0.5685, Test Loss: 0.5665, Train Acc: 0.7345, Test Acc: 0.7535, Precision: 0.5783, Recall: 0.7659\n",
      "Epoch: 287, Train Loss: 0.5685, Test Loss: 0.5646, Train Acc: 0.7349, Test Acc: 0.7597, Precision: 0.5818, Recall: 0.7691\n",
      "Epoch: 288, Train Loss: 0.5721, Test Loss: 0.5628, Train Acc: 0.7344, Test Acc: 0.7590, Precision: 0.5814, Recall: 0.7689\n",
      "Epoch: 289, Train Loss: 0.5720, Test Loss: 0.5636, Train Acc: 0.7342, Test Acc: 0.7536, Precision: 0.5787, Recall: 0.7691\n",
      "Epoch: 290, Train Loss: 0.5712, Test Loss: 0.5649, Train Acc: 0.7343, Test Acc: 0.7513, Precision: 0.5776, Recall: 0.7676\n",
      "Epoch: 291, Train Loss: 0.5691, Test Loss: 0.5645, Train Acc: 0.7349, Test Acc: 0.7547, Precision: 0.5790, Recall: 0.7678\n",
      "Epoch: 292, Train Loss: 0.5693, Test Loss: 0.5646, Train Acc: 0.7345, Test Acc: 0.7522, Precision: 0.5779, Recall: 0.7667\n",
      "Epoch: 293, Train Loss: 0.5688, Test Loss: 0.5675, Train Acc: 0.7340, Test Acc: 0.7517, Precision: 0.5776, Recall: 0.7659\n",
      "Epoch: 294, Train Loss: 0.5716, Test Loss: 0.5659, Train Acc: 0.7349, Test Acc: 0.7569, Precision: 0.5802, Recall: 0.7669\n",
      "Epoch: 295, Train Loss: 0.5672, Test Loss: 0.5631, Train Acc: 0.7356, Test Acc: 0.7555, Precision: 0.5797, Recall: 0.7680\n",
      "Epoch: 296, Train Loss: 0.5674, Test Loss: 0.5652, Train Acc: 0.7338, Test Acc: 0.7554, Precision: 0.5795, Recall: 0.7681\n",
      "Epoch: 297, Train Loss: 0.5683, Test Loss: 0.5634, Train Acc: 0.7351, Test Acc: 0.7564, Precision: 0.5799, Recall: 0.7687\n",
      "Epoch: 298, Train Loss: 0.5674, Test Loss: 0.5648, Train Acc: 0.7353, Test Acc: 0.7562, Precision: 0.5797, Recall: 0.7671\n",
      "Epoch: 299, Train Loss: 0.5680, Test Loss: 0.5642, Train Acc: 0.7359, Test Acc: 0.7565, Precision: 0.5800, Recall: 0.7687\n",
      "Epoch: 300, Train Loss: 0.5651, Test Loss: 0.5631, Train Acc: 0.7355, Test Acc: 0.7586, Precision: 0.5814, Recall: 0.7700\n",
      "Epoch: 301, Train Loss: 0.5679, Test Loss: 0.5663, Train Acc: 0.7357, Test Acc: 0.7582, Precision: 0.5809, Recall: 0.7680\n",
      "Epoch: 302, Train Loss: 0.5670, Test Loss: 0.5638, Train Acc: 0.7360, Test Acc: 0.7583, Precision: 0.5812, Recall: 0.7698\n",
      "Epoch: 303, Train Loss: 0.5669, Test Loss: 0.5661, Train Acc: 0.7349, Test Acc: 0.7564, Precision: 0.5801, Recall: 0.7688\n",
      "Epoch: 304, Train Loss: 0.5669, Test Loss: 0.5637, Train Acc: 0.7353, Test Acc: 0.7588, Precision: 0.5816, Recall: 0.7710\n",
      "Epoch: 305, Train Loss: 0.5671, Test Loss: 0.5647, Train Acc: 0.7342, Test Acc: 0.7593, Precision: 0.5815, Recall: 0.7689\n",
      "Epoch: 306, Train Loss: 0.5658, Test Loss: 0.5630, Train Acc: 0.7364, Test Acc: 0.7594, Precision: 0.5818, Recall: 0.7702\n",
      "Epoch: 307, Train Loss: 0.5665, Test Loss: 0.5660, Train Acc: 0.7354, Test Acc: 0.7512, Precision: 0.5776, Recall: 0.7666\n",
      "Epoch: 308, Train Loss: 0.5688, Test Loss: 0.5643, Train Acc: 0.7348, Test Acc: 0.7544, Precision: 0.5792, Recall: 0.7679\n",
      "Epoch: 309, Train Loss: 0.5685, Test Loss: 0.5638, Train Acc: 0.7346, Test Acc: 0.7561, Precision: 0.5798, Recall: 0.7680\n",
      "Epoch: 310, Train Loss: 0.5680, Test Loss: 0.5608, Train Acc: 0.7354, Test Acc: 0.7584, Precision: 0.5811, Recall: 0.7694\n",
      "Epoch: 311, Train Loss: 0.5671, Test Loss: 0.5644, Train Acc: 0.7360, Test Acc: 0.7613, Precision: 0.5829, Recall: 0.7695\n",
      "Epoch: 312, Train Loss: 0.5674, Test Loss: 0.5631, Train Acc: 0.7356, Test Acc: 0.7568, Precision: 0.5807, Recall: 0.7701\n",
      "Epoch: 313, Train Loss: 0.5668, Test Loss: 0.5656, Train Acc: 0.7353, Test Acc: 0.7571, Precision: 0.5807, Recall: 0.7701\n",
      "Epoch: 314, Train Loss: 0.5640, Test Loss: 0.5633, Train Acc: 0.7355, Test Acc: 0.7552, Precision: 0.5794, Recall: 0.7684\n",
      "Epoch: 315, Train Loss: 0.5639, Test Loss: 0.5638, Train Acc: 0.7353, Test Acc: 0.7548, Precision: 0.5793, Recall: 0.7682\n",
      "Epoch: 316, Train Loss: 0.5654, Test Loss: 0.5625, Train Acc: 0.7348, Test Acc: 0.7591, Precision: 0.5817, Recall: 0.7704\n",
      "Epoch: 317, Train Loss: 0.5654, Test Loss: 0.5625, Train Acc: 0.7354, Test Acc: 0.7584, Precision: 0.5814, Recall: 0.7706\n",
      "Epoch: 318, Train Loss: 0.5668, Test Loss: 0.5605, Train Acc: 0.7343, Test Acc: 0.7579, Precision: 0.5809, Recall: 0.7696\n",
      "Epoch: 319, Train Loss: 0.5660, Test Loss: 0.5611, Train Acc: 0.7347, Test Acc: 0.7592, Precision: 0.5821, Recall: 0.7722\n",
      "Epoch: 320, Train Loss: 0.5610, Test Loss: 0.5646, Train Acc: 0.7376, Test Acc: 0.7499, Precision: 0.5774, Recall: 0.7682\n",
      "Epoch: 321, Train Loss: 0.5638, Test Loss: 0.5604, Train Acc: 0.7345, Test Acc: 0.7602, Precision: 0.5824, Recall: 0.7711\n",
      "Epoch: 322, Train Loss: 0.5659, Test Loss: 0.5610, Train Acc: 0.7347, Test Acc: 0.7581, Precision: 0.5814, Recall: 0.7718\n",
      "Epoch: 323, Train Loss: 0.5644, Test Loss: 0.5630, Train Acc: 0.7346, Test Acc: 0.7589, Precision: 0.5817, Recall: 0.7710\n",
      "Epoch: 324, Train Loss: 0.5648, Test Loss: 0.5642, Train Acc: 0.7362, Test Acc: 0.7576, Precision: 0.5812, Recall: 0.7708\n",
      "Epoch: 325, Train Loss: 0.5614, Test Loss: 0.5619, Train Acc: 0.7349, Test Acc: 0.7582, Precision: 0.5815, Recall: 0.7717\n",
      "Epoch: 326, Train Loss: 0.5608, Test Loss: 0.5630, Train Acc: 0.7370, Test Acc: 0.7530, Precision: 0.5788, Recall: 0.7694\n",
      "Epoch: 327, Train Loss: 0.5647, Test Loss: 0.5610, Train Acc: 0.7352, Test Acc: 0.7588, Precision: 0.5817, Recall: 0.7707\n",
      "Epoch: 328, Train Loss: 0.5612, Test Loss: 0.5615, Train Acc: 0.7368, Test Acc: 0.7592, Precision: 0.5821, Recall: 0.7722\n",
      "Epoch: 329, Train Loss: 0.5611, Test Loss: 0.5608, Train Acc: 0.7359, Test Acc: 0.7587, Precision: 0.5817, Recall: 0.7716\n",
      "Epoch: 330, Train Loss: 0.5628, Test Loss: 0.5622, Train Acc: 0.7343, Test Acc: 0.7628, Precision: 0.5841, Recall: 0.7730\n",
      "Epoch: 331, Train Loss: 0.5647, Test Loss: 0.5595, Train Acc: 0.7355, Test Acc: 0.7585, Precision: 0.5817, Recall: 0.7717\n",
      "Epoch: 332, Train Loss: 0.5627, Test Loss: 0.5638, Train Acc: 0.7351, Test Acc: 0.7594, Precision: 0.5820, Recall: 0.7712\n",
      "Epoch: 333, Train Loss: 0.5591, Test Loss: 0.5619, Train Acc: 0.7358, Test Acc: 0.7575, Precision: 0.5811, Recall: 0.7713\n",
      "Epoch: 334, Train Loss: 0.5608, Test Loss: 0.5631, Train Acc: 0.7363, Test Acc: 0.7596, Precision: 0.5821, Recall: 0.7711\n",
      "Epoch: 335, Train Loss: 0.5625, Test Loss: 0.5605, Train Acc: 0.7358, Test Acc: 0.7611, Precision: 0.5830, Recall: 0.7716\n",
      "Epoch: 336, Train Loss: 0.5610, Test Loss: 0.5622, Train Acc: 0.7342, Test Acc: 0.7590, Precision: 0.5820, Recall: 0.7723\n",
      "Epoch: 337, Train Loss: 0.5590, Test Loss: 0.5637, Train Acc: 0.7343, Test Acc: 0.7563, Precision: 0.5803, Recall: 0.7702\n",
      "Epoch: 338, Train Loss: 0.5609, Test Loss: 0.5618, Train Acc: 0.7351, Test Acc: 0.7527, Precision: 0.5785, Recall: 0.7689\n",
      "Epoch: 339, Train Loss: 0.5623, Test Loss: 0.5623, Train Acc: 0.7343, Test Acc: 0.7594, Precision: 0.5822, Recall: 0.7721\n",
      "Epoch: 340, Train Loss: 0.5565, Test Loss: 0.5623, Train Acc: 0.7359, Test Acc: 0.7591, Precision: 0.5820, Recall: 0.7718\n",
      "Epoch: 341, Train Loss: 0.5618, Test Loss: 0.5627, Train Acc: 0.7337, Test Acc: 0.7576, Precision: 0.5813, Recall: 0.7718\n",
      "Epoch: 342, Train Loss: 0.5619, Test Loss: 0.5609, Train Acc: 0.7349, Test Acc: 0.7550, Precision: 0.5799, Recall: 0.7703\n",
      "Epoch: 343, Train Loss: 0.5628, Test Loss: 0.5619, Train Acc: 0.7321, Test Acc: 0.7573, Precision: 0.5809, Recall: 0.7710\n",
      "Epoch: 344, Train Loss: 0.5581, Test Loss: 0.5629, Train Acc: 0.7348, Test Acc: 0.7592, Precision: 0.5820, Recall: 0.7719\n",
      "Epoch: 345, Train Loss: 0.5612, Test Loss: 0.5620, Train Acc: 0.7357, Test Acc: 0.7588, Precision: 0.5819, Recall: 0.7725\n",
      "Epoch: 346, Train Loss: 0.5581, Test Loss: 0.5614, Train Acc: 0.7363, Test Acc: 0.7576, Precision: 0.5810, Recall: 0.7706\n",
      "Epoch: 347, Train Loss: 0.5588, Test Loss: 0.5622, Train Acc: 0.7353, Test Acc: 0.7585, Precision: 0.5816, Recall: 0.7717\n",
      "Epoch: 348, Train Loss: 0.5593, Test Loss: 0.5622, Train Acc: 0.7346, Test Acc: 0.7546, Precision: 0.5794, Recall: 0.7692\n",
      "Epoch: 349, Train Loss: 0.5600, Test Loss: 0.5621, Train Acc: 0.7355, Test Acc: 0.7602, Precision: 0.5823, Recall: 0.7707\n",
      "Epoch: 350, Train Loss: 0.5585, Test Loss: 0.5639, Train Acc: 0.7355, Test Acc: 0.7615, Precision: 0.5833, Recall: 0.7721\n",
      "Epoch: 351, Train Loss: 0.5593, Test Loss: 0.5617, Train Acc: 0.7353, Test Acc: 0.7579, Precision: 0.5814, Recall: 0.7712\n",
      "Epoch: 352, Train Loss: 0.5589, Test Loss: 0.5621, Train Acc: 0.7352, Test Acc: 0.7600, Precision: 0.5826, Recall: 0.7728\n",
      "Epoch: 353, Train Loss: 0.5630, Test Loss: 0.5645, Train Acc: 0.7337, Test Acc: 0.7597, Precision: 0.5824, Recall: 0.7721\n",
      "Epoch: 354, Train Loss: 0.5575, Test Loss: 0.5615, Train Acc: 0.7357, Test Acc: 0.7578, Precision: 0.5812, Recall: 0.7716\n",
      "Epoch: 355, Train Loss: 0.5588, Test Loss: 0.5629, Train Acc: 0.7354, Test Acc: 0.7602, Precision: 0.5826, Recall: 0.7717\n",
      "Epoch: 356, Train Loss: 0.5567, Test Loss: 0.5613, Train Acc: 0.7366, Test Acc: 0.7586, Precision: 0.5817, Recall: 0.7715\n",
      "Epoch: 357, Train Loss: 0.5538, Test Loss: 0.5612, Train Acc: 0.7356, Test Acc: 0.7611, Precision: 0.5831, Recall: 0.7727\n",
      "Epoch: 358, Train Loss: 0.5589, Test Loss: 0.5638, Train Acc: 0.7361, Test Acc: 0.7601, Precision: 0.5827, Recall: 0.7730\n",
      "Epoch: 359, Train Loss: 0.5558, Test Loss: 0.5608, Train Acc: 0.7365, Test Acc: 0.7621, Precision: 0.5839, Recall: 0.7739\n",
      "Epoch: 360, Train Loss: 0.5576, Test Loss: 0.5638, Train Acc: 0.7349, Test Acc: 0.7600, Precision: 0.5827, Recall: 0.7730\n",
      "Epoch: 361, Train Loss: 0.5588, Test Loss: 0.5606, Train Acc: 0.7357, Test Acc: 0.7625, Precision: 0.5838, Recall: 0.7729\n",
      "Epoch: 362, Train Loss: 0.5557, Test Loss: 0.5625, Train Acc: 0.7359, Test Acc: 0.7623, Precision: 0.5840, Recall: 0.7736\n",
      "Epoch: 363, Train Loss: 0.5593, Test Loss: 0.5622, Train Acc: 0.7357, Test Acc: 0.7561, Precision: 0.5804, Recall: 0.7711\n",
      "Epoch: 364, Train Loss: 0.5578, Test Loss: 0.5622, Train Acc: 0.7363, Test Acc: 0.7574, Precision: 0.5811, Recall: 0.7711\n",
      "Epoch: 365, Train Loss: 0.5590, Test Loss: 0.5611, Train Acc: 0.7347, Test Acc: 0.7590, Precision: 0.5820, Recall: 0.7723\n",
      "Epoch: 366, Train Loss: 0.5579, Test Loss: 0.5625, Train Acc: 0.7342, Test Acc: 0.7596, Precision: 0.5823, Recall: 0.7724\n",
      "Epoch: 367, Train Loss: 0.5565, Test Loss: 0.5596, Train Acc: 0.7357, Test Acc: 0.7616, Precision: 0.5835, Recall: 0.7734\n",
      "Epoch: 368, Train Loss: 0.5572, Test Loss: 0.5613, Train Acc: 0.7361, Test Acc: 0.7604, Precision: 0.5827, Recall: 0.7721\n",
      "Epoch: 369, Train Loss: 0.5555, Test Loss: 0.5592, Train Acc: 0.7347, Test Acc: 0.7638, Precision: 0.5850, Recall: 0.7745\n",
      "Epoch: 370, Train Loss: 0.5568, Test Loss: 0.5605, Train Acc: 0.7366, Test Acc: 0.7540, Precision: 0.5798, Recall: 0.7722\n",
      "Epoch: 371, Train Loss: 0.5544, Test Loss: 0.5608, Train Acc: 0.7346, Test Acc: 0.7627, Precision: 0.5842, Recall: 0.7736\n",
      "Epoch: 372, Train Loss: 0.5587, Test Loss: 0.5603, Train Acc: 0.7346, Test Acc: 0.7555, Precision: 0.5801, Recall: 0.7710\n",
      "Epoch: 373, Train Loss: 0.5518, Test Loss: 0.5602, Train Acc: 0.7350, Test Acc: 0.7600, Precision: 0.5826, Recall: 0.7727\n",
      "Epoch: 374, Train Loss: 0.5521, Test Loss: 0.5617, Train Acc: 0.7361, Test Acc: 0.7569, Precision: 0.5807, Recall: 0.7703\n",
      "Epoch: 375, Train Loss: 0.5546, Test Loss: 0.5606, Train Acc: 0.7359, Test Acc: 0.7592, Precision: 0.5821, Recall: 0.7720\n",
      "Epoch: 376, Train Loss: 0.5534, Test Loss: 0.5630, Train Acc: 0.7355, Test Acc: 0.7542, Precision: 0.5797, Recall: 0.7712\n",
      "Epoch: 377, Train Loss: 0.5557, Test Loss: 0.5595, Train Acc: 0.7345, Test Acc: 0.7592, Precision: 0.5822, Recall: 0.7730\n",
      "Epoch: 378, Train Loss: 0.5530, Test Loss: 0.5621, Train Acc: 0.7361, Test Acc: 0.7563, Precision: 0.5806, Recall: 0.7717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Training and testing loop\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1001\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     train()\n\u001b[1;32m    102\u001b[0m     test_acc, test_loss, precision, recall \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes  \u001b[38;5;66;03m# Accumulate loss for this epoch.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "num_classes = 3\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = GCN(hidden_channels=64, num_classes=num_classes).to(device)  # Make sure GCN uses correct number of classes\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Calculate class weights inversely proportional to class frequencies\n",
    "class_weights = []\n",
    "for class_idx in range(num_classes):\n",
    "    # Get counts for this class\n",
    "    class_count = train_class_counts[class_idx]\n",
    "    # Calculate weight (inversely proportional to frequency)\n",
    "    if class_count > 0:\n",
    "        weight = train_total_nodes / (num_classes * class_count)\n",
    "    else:\n",
    "        weight = 1.0\n",
    "    class_weights.append(weight)\n",
    "\n",
    "# Normalize weights\n",
    "class_weights = [w / sum(class_weights) * num_classes for w in class_weights]\n",
    "class_weights_tensor = torch.tensor(class_weights, device=device)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Use weighted cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "losses = []  # Training losses\n",
    "test_losses = []  # Test losses\n",
    "accs = []  # Test accuracies\n",
    "precisions = []  # Test precisions\n",
    "recalls = []  # Test recalls\n",
    "train_accs = []  # Training accuracies\n",
    "\n",
    "# Define the training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)  # Move batch data to GPU\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        total_loss += loss.item() * data.num_nodes  # Accumulate loss for this epoch.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        # Calculate accuracy for node predictions\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct = int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        total_correct += correct\n",
    "        total_nodes += data.num_nodes\n",
    "    \n",
    "    avg_loss = total_loss / total_nodes  # Calculate average loss per node\n",
    "    acc = total_correct / total_nodes  # Calculate accuracy per node\n",
    "    train_accs.append(acc)  # Append accuracy to the list.\n",
    "    losses.append(avg_loss)  # Append the average loss to the list.\n",
    "\n",
    "# Define the testing loop\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_nodes = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation.\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)  # Move batch data to GPU\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out, data.y)  # Compute the loss for the batch.\n",
    "            total_loss += loss.item() * data.num_nodes  # Accumulate test loss.\n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct = int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            total_correct += correct\n",
    "            total_nodes += data.num_nodes\n",
    "\n",
    "            # Collect predictions and true labels for precision and recall\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_nodes  # Calculate average loss per node\n",
    "    test_losses.append(avg_loss)  # Append the average test loss to the list.\n",
    "    acc = total_correct / total_nodes  # Calculate accuracy per node\n",
    "    accs.append(acc)  # Append accuracy to the list.\n",
    "\n",
    "    # Compute precision and recall\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    return acc, avg_loss, precision, recall\n",
    "\n",
    "# Training and testing loop\n",
    "for epoch in range(1, 1001):\n",
    "    train()\n",
    "    test_acc, test_loss, precision, recall = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {losses[-1]:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_accs[-1]:.4f}, Test Acc: {test_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334853a-cd7f-4346-a99a-3b2f56400298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "227f2821-6520-405a-b862-d9d8ea644927",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1110016-0917-4e8d-9c07-d6d5f56fde85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (378,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the loss curve\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1001\u001b[39m), losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1001\u001b[39m), test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m   3591\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m   3592\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[1;32m   3593\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[1;32m   3594\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   3595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3596\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/axes/_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_args(\n\u001b[1;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[38;5;241m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (378,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(range(1, 1001), losses, label='Training Loss')\n",
    "plt.plot(range(1,1001), test_losses, label='Test Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.savefig('../Data/loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc70de-0dfe-4bc5-b5c1-bfaafba4e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 1001), accs, label='Test accuracy')\n",
    "# plt.plot(range(1, 101), train_accs, label='Train accuracy')\n",
    "print(f'Max accuracy: {max(accs)}')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves')\n",
    "# plt.savefig('../Data/accuracy_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea127ab-fef7-4ad4-ab82-caa9f996774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 1001), precisions, label='Precision')\n",
    "plt.plot(range(1, 1001), recalls, label='Recall')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.title('Precision and Recall Curves')\n",
    "plt.savefig('../Data/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbf68a-a8ea-406b-8b0f-ee1c27c23f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74ec10-10c1-4c14-8703-213ae4377ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdc312-c6b6-4482-b67a-8e85ac0cb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
