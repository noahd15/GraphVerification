{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6975ae06-82d8-4b20-a5b7-bb4296451d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from CybORG import CybORG\n",
    "# from CybORG.Simulator.Scenarios.DroneSwarmScenarioGenerator import DroneSwarmScenarioGenerator\n",
    "# from CybORG.Agents.SimpleAgents.RedDroneWorm import RedDroneWormAgent\n",
    "# from CybORG.Agents.Wrappers import PettingZooParallelWrapper\n",
    "\n",
    "import torch\n",
    "from torch_geometric.utils import from_networkx\n",
    "# from ipaddress import IPv4Address\n",
    "import random\n",
    "import logging\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd8be1-62eb-4bbf-bb71-be059a394214",
   "metadata": {},
   "source": [
    "# Train GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fe629",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dd179ed-bc79-4074-8b19-94bacd3a42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 14000\n",
      "Test dataset length: 6000\n"
     ]
    }
   ],
   "source": [
    "filename = './data/dataset_python_node'\n",
    "# testfile = '../Data/testmulti'\n",
    "\n",
    "\n",
    "\n",
    "combined_dataset = torch.load(f'{filename}.pt', weights_only=False)\n",
    "# testset = torch.load(f'{testfile}.pt', weights_only=False)\n",
    "\n",
    "\n",
    "\n",
    "# combined_dataset = dataset + testset\n",
    "# print(f\"Combined dataset length: {len(combined_dataset)}\")\n",
    "random.shuffle(combined_dataset)\n",
    "\n",
    "size = len(combined_dataset)\n",
    "train_size = int(0.7 * size)\n",
    "\n",
    "dataset = combined_dataset[:train_size]\n",
    "testset = combined_dataset[train_size:]\n",
    "print(f\"Train dataset length: {len(dataset)}\")\n",
    "print(f\"Test dataset length: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f73ac4-5548-4004-b36d-288e220c18a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000\n",
      "\n",
      "Data(edge_index=[2, 62], num_nodes=18, x=[18, 110], y=[18])\n",
      "=============================================================\n",
      "Number of nodes: 18\n",
      "Number of edges: 62\n",
      "Average node degree: 3.44\n",
      "Has isolated nodes: True\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "\n",
    "for data in dataset:\n",
    "    delattr(data, 'feature')\n",
    "\n",
    "\n",
    "print(len(dataset))\n",
    "data = dataset[10]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06fb4ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 14000\n",
      "Number of test graphs: 6000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# random.shuffle(dataset)\n",
    "# random.shuffle(testset)\n",
    "# train_dataset = dataset[:12000]\n",
    "# test_dataset = dataset[12000:]\n",
    "\n",
    "# size = len(dataset)\n",
    "# size = size - size//4\n",
    "\n",
    "train_dataset = dataset\n",
    "test_dataset = testset\n",
    "\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3273c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    if data.size(1) != 18:\n",
    "        print(f\"Warning: Data shape {data.shape} is not 18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "733f7409-47ab-4dd8-abc5-bf58ba570fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in train dataset: 110\n",
      "Number of classes detected: 3\n",
      "--------------Train--------------\n",
      "\u001b[1mTotal nodes: \u001b[0m252000\n",
      "\u001b[1mPercent of Not Compromised: \u001b[0m0.3535 (89076 nodes)\n",
      "\u001b[1mPercent of Compromised: \u001b[0m0.0155 (3894 nodes)\n",
      "\u001b[1mPercent of Highly Compromised: \u001b[0m0.6311 (159030 nodes)\n",
      "\n",
      "--------------Test--------------\n",
      "\u001b[1mTotal nodes: \u001b[0m108000\n",
      "\u001b[1mPercent of Not Compromised: \u001b[0m0.3476 (37541 nodes)\n",
      "\u001b[1mPercent of Compromised: \u001b[0m0.0155 (1671 nodes)\n",
      "\u001b[1mPercent of Highly Compromised: \u001b[0m0.6369 (68788 nodes)\n",
      "\n",
      "Class distribution:\n",
      "Not Compromised: █████████████████ 35.35%\n",
      "Compromised:  1.55%\n",
      "Highly Compromised: ███████████████████████████████ 63.11%\n"
     ]
    }
   ],
   "source": [
    "# test_dataset = torch.load(f\"test_gl_selected.pt\", weights_only=False)\n",
    "# train_dataset = torch.load(f\"train_gl_selected.pt\", weights_only=False)\n",
    "\n",
    "test_dataset = testset\n",
    "train_dataset = dataset\n",
    "\n",
    "print(f'Number of features in train dataset: {train_dataset[0].x.shape[1]}')\n",
    "\n",
    "# Determine the number of classes in the dataset\n",
    "unique_classes = set()\n",
    "for t in train_dataset:\n",
    "    unique_classes.update(t.y.unique().cpu().numpy())\n",
    "num_classes = len(unique_classes)\n",
    "print(f\"Number of classes detected: {num_classes}\")\n",
    "\n",
    "# Test dataset statistics\n",
    "test_total_nodes = 0\n",
    "test_class_counts = {i: 0 for i in range(num_classes)}\n",
    "\n",
    "for t in test_dataset:\n",
    "    test_total_nodes += t.y.size(0)\n",
    "    for class_idx in range(num_classes):\n",
    "        test_class_counts[class_idx] += (t.y == class_idx).sum().item()\n",
    "\n",
    "# Train dataset statistics\n",
    "train_total_nodes = 0\n",
    "train_class_counts = {i: 0 for i in range(num_classes)}\n",
    "\n",
    "for t in train_dataset:\n",
    "    train_total_nodes += t.y.size(0)\n",
    "    for class_idx in range(num_classes):\n",
    "        train_class_counts[class_idx] += (t.y == class_idx).sum().item()\n",
    "\n",
    "# Print statistics with class labels\n",
    "class_labels = [\"Not Compromised\", \"Compromised\", \"Highly Compromised\"] if num_classes == 3 else [f\"Class {i}\" for i in range(num_classes)]\n",
    "\n",
    "print('--------------Train--------------')\n",
    "print(f'\\033[1mTotal nodes: \\033[0m{train_total_nodes}')\n",
    "for i in range(num_classes):\n",
    "    percentage = train_class_counts[i]/train_total_nodes if train_total_nodes > 0 else 0\n",
    "    print(f'\\033[1mPercent of {class_labels[i]}: \\033[0m{percentage:.4f} ({train_class_counts[i]} nodes)')\n",
    "\n",
    "print('\\n--------------Test--------------')\n",
    "print(f'\\033[1mTotal nodes: \\033[0m{test_total_nodes}')\n",
    "for i in range(num_classes):\n",
    "    percentage = test_class_counts[i]/test_total_nodes if test_total_nodes > 0 else 0\n",
    "    print(f'\\033[1mPercent of {class_labels[i]}: \\033[0m{percentage:.4f} ({test_class_counts[i]} nodes)')\n",
    "\n",
    "# Print class distribution visualization if desired\n",
    "print(\"\\nClass distribution:\")\n",
    "max_bar_length = 50  # maximum bar length in characters\n",
    "for i in range(num_classes):\n",
    "    train_pct = train_class_counts[i]/train_total_nodes if train_total_nodes > 0 else 0\n",
    "    bar_length = int(train_pct * max_bar_length)\n",
    "    print(f\"{class_labels[i]}: {'█' * bar_length} {train_pct:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ca1f5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "282f09a1-471b-4af4-b740-62a1642ea5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "GCN(\n",
      "  (conv1): GCNConv(110, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (conv3): GCNConv(32, 32)\n",
      "  (lin): Linear(in_features=32, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Specify device manually\n",
    "use_gpu = True # Set to False to use CPU\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# DataLoader (ensure `train_dataset` and `test_dataset` are loaded correctly)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "input_dim = len(train_dataset[0].x[0])\n",
    "\n",
    "# Define the GCN model for node classification\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim=input_dim,hidden_channels=32,num_classes=3):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(input_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, num_classes)  # 2 classes: compromised or not\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        # 2. Node-level classification\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b50f932-1389-42a8-af60-38bdc410cd79",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.12277313406033144, 2.808459088227551, 0.06876777771211773]\n",
      "Epoch: 001, Train Loss: 2.5530, Test Loss: 0.9034, Train Acc: 0.4589, Test Acc: 0.6823, Precision: 0.5448, Recall: 0.5833\n",
      "Epoch: 002, Train Loss: 1.2374, Test Loss: 0.8667, Train Acc: 0.5318, Test Acc: 0.6512, Precision: 0.5609, Recall: 0.5938\n",
      "Epoch: 003, Train Loss: 1.0477, Test Loss: 0.8564, Train Acc: 0.5616, Test Acc: 0.6299, Precision: 0.5708, Recall: 0.6002\n",
      "Epoch: 004, Train Loss: 0.9740, Test Loss: 0.8443, Train Acc: 0.5805, Test Acc: 0.6226, Precision: 0.5764, Recall: 0.6121\n",
      "Epoch: 005, Train Loss: 0.9372, Test Loss: 0.8353, Train Acc: 0.5869, Test Acc: 0.6244, Precision: 0.5796, Recall: 0.6215\n",
      "Epoch: 006, Train Loss: 0.9102, Test Loss: 0.8275, Train Acc: 0.5981, Test Acc: 0.6230, Precision: 0.5817, Recall: 0.6274\n",
      "Epoch: 007, Train Loss: 0.8928, Test Loss: 0.8233, Train Acc: 0.6038, Test Acc: 0.6216, Precision: 0.5838, Recall: 0.6307\n",
      "Epoch: 008, Train Loss: 0.8851, Test Loss: 0.8198, Train Acc: 0.6002, Test Acc: 0.6180, Precision: 0.5864, Recall: 0.6314\n",
      "Epoch: 009, Train Loss: 0.8695, Test Loss: 0.8110, Train Acc: 0.6028, Test Acc: 0.6147, Precision: 0.5868, Recall: 0.6346\n",
      "Epoch: 010, Train Loss: 0.8644, Test Loss: 0.8093, Train Acc: 0.6019, Test Acc: 0.6155, Precision: 0.5866, Recall: 0.6362\n",
      "Epoch: 011, Train Loss: 0.8545, Test Loss: 0.8040, Train Acc: 0.6066, Test Acc: 0.6048, Precision: 0.5911, Recall: 0.6372\n",
      "Epoch: 012, Train Loss: 0.8465, Test Loss: 0.7982, Train Acc: 0.6072, Test Acc: 0.6129, Precision: 0.5894, Recall: 0.6395\n",
      "Epoch: 013, Train Loss: 0.8370, Test Loss: 0.7937, Train Acc: 0.6104, Test Acc: 0.6154, Precision: 0.5877, Recall: 0.6394\n",
      "Epoch: 014, Train Loss: 0.8294, Test Loss: 0.7873, Train Acc: 0.6108, Test Acc: 0.6165, Precision: 0.5873, Recall: 0.6400\n",
      "Epoch: 015, Train Loss: 0.8262, Test Loss: 0.7842, Train Acc: 0.6160, Test Acc: 0.6175, Precision: 0.5892, Recall: 0.6419\n",
      "Epoch: 016, Train Loss: 0.8169, Test Loss: 0.7792, Train Acc: 0.6166, Test Acc: 0.6300, Precision: 0.5854, Recall: 0.6447\n",
      "Epoch: 017, Train Loss: 0.8133, Test Loss: 0.7758, Train Acc: 0.6206, Test Acc: 0.6302, Precision: 0.5855, Recall: 0.6452\n",
      "Epoch: 018, Train Loss: 0.8111, Test Loss: 0.7737, Train Acc: 0.6173, Test Acc: 0.6332, Precision: 0.5866, Recall: 0.6457\n",
      "Epoch: 019, Train Loss: 0.8053, Test Loss: 0.7680, Train Acc: 0.6260, Test Acc: 0.6423, Precision: 0.5836, Recall: 0.6472\n",
      "Epoch: 020, Train Loss: 0.8002, Test Loss: 0.7654, Train Acc: 0.6320, Test Acc: 0.6369, Precision: 0.5865, Recall: 0.6510\n",
      "Epoch: 021, Train Loss: 0.7996, Test Loss: 0.7627, Train Acc: 0.6319, Test Acc: 0.6489, Precision: 0.5822, Recall: 0.6512\n",
      "Epoch: 022, Train Loss: 0.7916, Test Loss: 0.7582, Train Acc: 0.6352, Test Acc: 0.6420, Precision: 0.5843, Recall: 0.6518\n",
      "Epoch: 023, Train Loss: 0.7888, Test Loss: 0.7552, Train Acc: 0.6358, Test Acc: 0.6517, Precision: 0.5808, Recall: 0.6519\n",
      "Epoch: 024, Train Loss: 0.7854, Test Loss: 0.7527, Train Acc: 0.6394, Test Acc: 0.6431, Precision: 0.5844, Recall: 0.6526\n",
      "Epoch: 025, Train Loss: 0.7806, Test Loss: 0.7508, Train Acc: 0.6400, Test Acc: 0.6714, Precision: 0.5774, Recall: 0.6622\n",
      "Epoch: 026, Train Loss: 0.7789, Test Loss: 0.7449, Train Acc: 0.6462, Test Acc: 0.6535, Precision: 0.5804, Recall: 0.6563\n",
      "Epoch: 027, Train Loss: 0.7745, Test Loss: 0.7428, Train Acc: 0.6438, Test Acc: 0.6646, Precision: 0.5783, Recall: 0.6613\n",
      "Epoch: 028, Train Loss: 0.7707, Test Loss: 0.7384, Train Acc: 0.6487, Test Acc: 0.6599, Precision: 0.5806, Recall: 0.6633\n",
      "Epoch: 029, Train Loss: 0.7699, Test Loss: 0.7383, Train Acc: 0.6491, Test Acc: 0.6652, Precision: 0.5799, Recall: 0.6681\n",
      "Epoch: 030, Train Loss: 0.7666, Test Loss: 0.7340, Train Acc: 0.6518, Test Acc: 0.6706, Precision: 0.5800, Recall: 0.6723\n",
      "Epoch: 031, Train Loss: 0.7614, Test Loss: 0.7299, Train Acc: 0.6563, Test Acc: 0.6744, Precision: 0.5778, Recall: 0.6728\n",
      "Epoch: 032, Train Loss: 0.7630, Test Loss: 0.7280, Train Acc: 0.6544, Test Acc: 0.6666, Precision: 0.5801, Recall: 0.6774\n",
      "Epoch: 033, Train Loss: 0.7593, Test Loss: 0.7274, Train Acc: 0.6542, Test Acc: 0.6780, Precision: 0.5785, Recall: 0.6781\n",
      "Epoch: 034, Train Loss: 0.7535, Test Loss: 0.7226, Train Acc: 0.6620, Test Acc: 0.6828, Precision: 0.5780, Recall: 0.6854\n",
      "Epoch: 035, Train Loss: 0.7524, Test Loss: 0.7191, Train Acc: 0.6620, Test Acc: 0.6882, Precision: 0.5777, Recall: 0.6864\n",
      "Epoch: 036, Train Loss: 0.7490, Test Loss: 0.7149, Train Acc: 0.6645, Test Acc: 0.6791, Precision: 0.5790, Recall: 0.6887\n",
      "Epoch: 037, Train Loss: 0.7496, Test Loss: 0.7123, Train Acc: 0.6632, Test Acc: 0.6863, Precision: 0.5781, Recall: 0.6896\n",
      "Epoch: 038, Train Loss: 0.7428, Test Loss: 0.7105, Train Acc: 0.6653, Test Acc: 0.6865, Precision: 0.5773, Recall: 0.6921\n",
      "Epoch: 039, Train Loss: 0.7413, Test Loss: 0.7075, Train Acc: 0.6641, Test Acc: 0.6820, Precision: 0.5789, Recall: 0.6923\n",
      "Epoch: 040, Train Loss: 0.7383, Test Loss: 0.7044, Train Acc: 0.6686, Test Acc: 0.6819, Precision: 0.5804, Recall: 0.6954\n",
      "Epoch: 041, Train Loss: 0.7354, Test Loss: 0.7003, Train Acc: 0.6660, Test Acc: 0.6866, Precision: 0.5765, Recall: 0.6987\n",
      "Epoch: 042, Train Loss: 0.7342, Test Loss: 0.6981, Train Acc: 0.6737, Test Acc: 0.6896, Precision: 0.5764, Recall: 0.7017\n",
      "Epoch: 043, Train Loss: 0.7313, Test Loss: 0.6992, Train Acc: 0.6712, Test Acc: 0.7034, Precision: 0.5744, Recall: 0.7031\n",
      "Epoch: 044, Train Loss: 0.7301, Test Loss: 0.6943, Train Acc: 0.6737, Test Acc: 0.7012, Precision: 0.5752, Recall: 0.7047\n",
      "Epoch: 045, Train Loss: 0.7259, Test Loss: 0.6919, Train Acc: 0.6759, Test Acc: 0.6939, Precision: 0.5756, Recall: 0.7075\n",
      "Epoch: 046, Train Loss: 0.7224, Test Loss: 0.6867, Train Acc: 0.6773, Test Acc: 0.6982, Precision: 0.5745, Recall: 0.7094\n",
      "Epoch: 047, Train Loss: 0.7213, Test Loss: 0.6850, Train Acc: 0.6794, Test Acc: 0.6884, Precision: 0.5762, Recall: 0.7080\n",
      "Epoch: 048, Train Loss: 0.7186, Test Loss: 0.6828, Train Acc: 0.6782, Test Acc: 0.6985, Precision: 0.5737, Recall: 0.7133\n",
      "Epoch: 049, Train Loss: 0.7181, Test Loss: 0.6801, Train Acc: 0.6784, Test Acc: 0.7064, Precision: 0.5724, Recall: 0.7129\n",
      "Epoch: 050, Train Loss: 0.7133, Test Loss: 0.6771, Train Acc: 0.6813, Test Acc: 0.6999, Precision: 0.5743, Recall: 0.7138\n",
      "Epoch: 051, Train Loss: 0.7149, Test Loss: 0.6748, Train Acc: 0.6840, Test Acc: 0.6893, Precision: 0.5749, Recall: 0.7151\n",
      "Epoch: 052, Train Loss: 0.7085, Test Loss: 0.6739, Train Acc: 0.6851, Test Acc: 0.7077, Precision: 0.5726, Recall: 0.7180\n",
      "Epoch: 053, Train Loss: 0.7079, Test Loss: 0.6718, Train Acc: 0.6851, Test Acc: 0.7138, Precision: 0.5725, Recall: 0.7188\n",
      "Epoch: 054, Train Loss: 0.7057, Test Loss: 0.6697, Train Acc: 0.6853, Test Acc: 0.7091, Precision: 0.5726, Recall: 0.7194\n",
      "Epoch: 055, Train Loss: 0.7083, Test Loss: 0.6665, Train Acc: 0.6857, Test Acc: 0.7028, Precision: 0.5732, Recall: 0.7205\n",
      "Epoch: 056, Train Loss: 0.7010, Test Loss: 0.6643, Train Acc: 0.6873, Test Acc: 0.7069, Precision: 0.5728, Recall: 0.7222\n",
      "Epoch: 057, Train Loss: 0.7029, Test Loss: 0.6628, Train Acc: 0.6881, Test Acc: 0.7014, Precision: 0.5730, Recall: 0.7232\n",
      "Epoch: 058, Train Loss: 0.7021, Test Loss: 0.6628, Train Acc: 0.6877, Test Acc: 0.7031, Precision: 0.5725, Recall: 0.7233\n",
      "Epoch: 059, Train Loss: 0.6942, Test Loss: 0.6595, Train Acc: 0.6891, Test Acc: 0.7125, Precision: 0.5732, Recall: 0.7236\n",
      "Epoch: 060, Train Loss: 0.6973, Test Loss: 0.6574, Train Acc: 0.6902, Test Acc: 0.7024, Precision: 0.5728, Recall: 0.7235\n",
      "Epoch: 061, Train Loss: 0.6919, Test Loss: 0.6556, Train Acc: 0.6899, Test Acc: 0.7187, Precision: 0.5721, Recall: 0.7282\n",
      "Epoch: 062, Train Loss: 0.6914, Test Loss: 0.6533, Train Acc: 0.6905, Test Acc: 0.7085, Precision: 0.5722, Recall: 0.7279\n",
      "Epoch: 063, Train Loss: 0.6914, Test Loss: 0.6522, Train Acc: 0.6933, Test Acc: 0.7103, Precision: 0.5714, Recall: 0.7290\n",
      "Epoch: 064, Train Loss: 0.6887, Test Loss: 0.6512, Train Acc: 0.6923, Test Acc: 0.7144, Precision: 0.5720, Recall: 0.7303\n",
      "Epoch: 065, Train Loss: 0.6859, Test Loss: 0.6495, Train Acc: 0.6940, Test Acc: 0.7127, Precision: 0.5727, Recall: 0.7295\n",
      "Epoch: 066, Train Loss: 0.6927, Test Loss: 0.6517, Train Acc: 0.6945, Test Acc: 0.7188, Precision: 0.5707, Recall: 0.7307\n",
      "Epoch: 067, Train Loss: 0.6873, Test Loss: 0.6483, Train Acc: 0.6963, Test Acc: 0.7184, Precision: 0.5723, Recall: 0.7325\n",
      "Epoch: 068, Train Loss: 0.6845, Test Loss: 0.6463, Train Acc: 0.6970, Test Acc: 0.7190, Precision: 0.5720, Recall: 0.7335\n",
      "Epoch: 069, Train Loss: 0.6864, Test Loss: 0.6437, Train Acc: 0.6959, Test Acc: 0.7161, Precision: 0.5716, Recall: 0.7317\n",
      "Epoch: 070, Train Loss: 0.6801, Test Loss: 0.6445, Train Acc: 0.6984, Test Acc: 0.7225, Precision: 0.5704, Recall: 0.7341\n",
      "Epoch: 071, Train Loss: 0.6758, Test Loss: 0.6427, Train Acc: 0.7004, Test Acc: 0.7271, Precision: 0.5706, Recall: 0.7344\n",
      "Epoch: 072, Train Loss: 0.6794, Test Loss: 0.6388, Train Acc: 0.7001, Test Acc: 0.7182, Precision: 0.5710, Recall: 0.7353\n",
      "Epoch: 073, Train Loss: 0.6797, Test Loss: 0.6384, Train Acc: 0.6985, Test Acc: 0.7166, Precision: 0.5721, Recall: 0.7357\n",
      "Epoch: 074, Train Loss: 0.6745, Test Loss: 0.6375, Train Acc: 0.7011, Test Acc: 0.7192, Precision: 0.5713, Recall: 0.7375\n",
      "Epoch: 075, Train Loss: 0.6739, Test Loss: 0.6355, Train Acc: 0.7012, Test Acc: 0.7175, Precision: 0.5713, Recall: 0.7353\n",
      "Epoch: 076, Train Loss: 0.6710, Test Loss: 0.6347, Train Acc: 0.7027, Test Acc: 0.7247, Precision: 0.5714, Recall: 0.7368\n",
      "Epoch: 077, Train Loss: 0.6718, Test Loss: 0.6357, Train Acc: 0.7021, Test Acc: 0.7227, Precision: 0.5716, Recall: 0.7373\n",
      "Epoch: 078, Train Loss: 0.6713, Test Loss: 0.6351, Train Acc: 0.7036, Test Acc: 0.7227, Precision: 0.5705, Recall: 0.7360\n",
      "Epoch: 079, Train Loss: 0.6680, Test Loss: 0.6329, Train Acc: 0.7055, Test Acc: 0.7239, Precision: 0.5713, Recall: 0.7387\n",
      "Epoch: 080, Train Loss: 0.6676, Test Loss: 0.6299, Train Acc: 0.7052, Test Acc: 0.7225, Precision: 0.5714, Recall: 0.7385\n",
      "Epoch: 081, Train Loss: 0.6668, Test Loss: 0.6318, Train Acc: 0.7040, Test Acc: 0.7325, Precision: 0.5721, Recall: 0.7381\n",
      "Epoch: 082, Train Loss: 0.6681, Test Loss: 0.6303, Train Acc: 0.7057, Test Acc: 0.7230, Precision: 0.5721, Recall: 0.7405\n",
      "Epoch: 083, Train Loss: 0.6656, Test Loss: 0.6272, Train Acc: 0.7058, Test Acc: 0.7257, Precision: 0.5712, Recall: 0.7407\n",
      "Epoch: 084, Train Loss: 0.6639, Test Loss: 0.6264, Train Acc: 0.7073, Test Acc: 0.7285, Precision: 0.5721, Recall: 0.7416\n",
      "Epoch: 085, Train Loss: 0.6609, Test Loss: 0.6253, Train Acc: 0.7083, Test Acc: 0.7209, Precision: 0.5709, Recall: 0.7401\n",
      "Epoch: 086, Train Loss: 0.6591, Test Loss: 0.6242, Train Acc: 0.7098, Test Acc: 0.7282, Precision: 0.5717, Recall: 0.7417\n",
      "Epoch: 087, Train Loss: 0.6596, Test Loss: 0.6239, Train Acc: 0.7123, Test Acc: 0.7205, Precision: 0.5707, Recall: 0.7387\n",
      "Epoch: 088, Train Loss: 0.6586, Test Loss: 0.6216, Train Acc: 0.7097, Test Acc: 0.7255, Precision: 0.5710, Recall: 0.7411\n",
      "Epoch: 089, Train Loss: 0.6604, Test Loss: 0.6223, Train Acc: 0.7109, Test Acc: 0.7263, Precision: 0.5718, Recall: 0.7427\n",
      "Epoch: 090, Train Loss: 0.6583, Test Loss: 0.6204, Train Acc: 0.7103, Test Acc: 0.7215, Precision: 0.5711, Recall: 0.7413\n",
      "Epoch: 091, Train Loss: 0.6570, Test Loss: 0.6202, Train Acc: 0.7112, Test Acc: 0.7255, Precision: 0.5711, Recall: 0.7420\n",
      "Epoch: 092, Train Loss: 0.6524, Test Loss: 0.6195, Train Acc: 0.7121, Test Acc: 0.7193, Precision: 0.5702, Recall: 0.7403\n",
      "Epoch: 093, Train Loss: 0.6548, Test Loss: 0.6186, Train Acc: 0.7119, Test Acc: 0.7289, Precision: 0.5713, Recall: 0.7418\n",
      "Epoch: 094, Train Loss: 0.6531, Test Loss: 0.6180, Train Acc: 0.7131, Test Acc: 0.7297, Precision: 0.5716, Recall: 0.7424\n",
      "Epoch: 095, Train Loss: 0.6511, Test Loss: 0.6162, Train Acc: 0.7133, Test Acc: 0.7286, Precision: 0.5700, Recall: 0.7428\n",
      "Epoch: 096, Train Loss: 0.6477, Test Loss: 0.6171, Train Acc: 0.7141, Test Acc: 0.7222, Precision: 0.5695, Recall: 0.7424\n",
      "Epoch: 097, Train Loss: 0.6498, Test Loss: 0.6156, Train Acc: 0.7145, Test Acc: 0.7275, Precision: 0.5707, Recall: 0.7420\n",
      "Epoch: 098, Train Loss: 0.6487, Test Loss: 0.6130, Train Acc: 0.7142, Test Acc: 0.7267, Precision: 0.5710, Recall: 0.7439\n",
      "Epoch: 099, Train Loss: 0.6475, Test Loss: 0.6140, Train Acc: 0.7149, Test Acc: 0.7256, Precision: 0.5718, Recall: 0.7436\n",
      "Epoch: 100, Train Loss: 0.6445, Test Loss: 0.6124, Train Acc: 0.7150, Test Acc: 0.7291, Precision: 0.5717, Recall: 0.7451\n",
      "Epoch: 101, Train Loss: 0.6472, Test Loss: 0.6117, Train Acc: 0.7160, Test Acc: 0.7254, Precision: 0.5724, Recall: 0.7436\n",
      "Epoch: 102, Train Loss: 0.6462, Test Loss: 0.6101, Train Acc: 0.7152, Test Acc: 0.7287, Precision: 0.5719, Recall: 0.7446\n",
      "Epoch: 103, Train Loss: 0.6431, Test Loss: 0.6108, Train Acc: 0.7161, Test Acc: 0.7253, Precision: 0.5706, Recall: 0.7429\n",
      "Epoch: 104, Train Loss: 0.6456, Test Loss: 0.6108, Train Acc: 0.7163, Test Acc: 0.7287, Precision: 0.5710, Recall: 0.7459\n",
      "Epoch: 105, Train Loss: 0.6438, Test Loss: 0.6095, Train Acc: 0.7164, Test Acc: 0.7289, Precision: 0.5710, Recall: 0.7444\n",
      "Epoch: 106, Train Loss: 0.6428, Test Loss: 0.6091, Train Acc: 0.7154, Test Acc: 0.7293, Precision: 0.5717, Recall: 0.7460\n",
      "Epoch: 107, Train Loss: 0.6425, Test Loss: 0.6071, Train Acc: 0.7187, Test Acc: 0.7270, Precision: 0.5712, Recall: 0.7451\n",
      "Epoch: 108, Train Loss: 0.6415, Test Loss: 0.6071, Train Acc: 0.7177, Test Acc: 0.7311, Precision: 0.5714, Recall: 0.7467\n",
      "Epoch: 109, Train Loss: 0.6407, Test Loss: 0.6074, Train Acc: 0.7193, Test Acc: 0.7310, Precision: 0.5707, Recall: 0.7461\n",
      "Epoch: 110, Train Loss: 0.6366, Test Loss: 0.6053, Train Acc: 0.7185, Test Acc: 0.7320, Precision: 0.5711, Recall: 0.7455\n",
      "Epoch: 111, Train Loss: 0.6375, Test Loss: 0.6058, Train Acc: 0.7184, Test Acc: 0.7295, Precision: 0.5704, Recall: 0.7466\n",
      "Epoch: 112, Train Loss: 0.6379, Test Loss: 0.6042, Train Acc: 0.7186, Test Acc: 0.7309, Precision: 0.5712, Recall: 0.7474\n",
      "Epoch: 113, Train Loss: 0.6399, Test Loss: 0.6032, Train Acc: 0.7174, Test Acc: 0.7309, Precision: 0.5721, Recall: 0.7471\n",
      "Epoch: 114, Train Loss: 0.6387, Test Loss: 0.6028, Train Acc: 0.7198, Test Acc: 0.7282, Precision: 0.5705, Recall: 0.7474\n",
      "Epoch: 115, Train Loss: 0.6332, Test Loss: 0.6025, Train Acc: 0.7193, Test Acc: 0.7365, Precision: 0.5724, Recall: 0.7475\n",
      "Epoch: 116, Train Loss: 0.6372, Test Loss: 0.6013, Train Acc: 0.7200, Test Acc: 0.7304, Precision: 0.5718, Recall: 0.7470\n",
      "Epoch: 117, Train Loss: 0.6373, Test Loss: 0.6022, Train Acc: 0.7188, Test Acc: 0.7323, Precision: 0.5711, Recall: 0.7492\n",
      "Epoch: 118, Train Loss: 0.6357, Test Loss: 0.5992, Train Acc: 0.7209, Test Acc: 0.7330, Precision: 0.5716, Recall: 0.7500\n",
      "Epoch: 119, Train Loss: 0.6346, Test Loss: 0.6001, Train Acc: 0.7207, Test Acc: 0.7342, Precision: 0.5705, Recall: 0.7482\n",
      "Epoch: 120, Train Loss: 0.6335, Test Loss: 0.5997, Train Acc: 0.7210, Test Acc: 0.7341, Precision: 0.5711, Recall: 0.7496\n",
      "Epoch: 121, Train Loss: 0.6324, Test Loss: 0.5990, Train Acc: 0.7215, Test Acc: 0.7324, Precision: 0.5713, Recall: 0.7493\n",
      "Epoch: 122, Train Loss: 0.6320, Test Loss: 0.5991, Train Acc: 0.7215, Test Acc: 0.7364, Precision: 0.5719, Recall: 0.7500\n",
      "Epoch: 123, Train Loss: 0.6308, Test Loss: 0.5971, Train Acc: 0.7222, Test Acc: 0.7377, Precision: 0.5725, Recall: 0.7516\n",
      "Epoch: 124, Train Loss: 0.6288, Test Loss: 0.5984, Train Acc: 0.7234, Test Acc: 0.7274, Precision: 0.5706, Recall: 0.7469\n",
      "Epoch: 125, Train Loss: 0.6314, Test Loss: 0.5979, Train Acc: 0.7224, Test Acc: 0.7361, Precision: 0.5718, Recall: 0.7496\n",
      "Epoch: 126, Train Loss: 0.6310, Test Loss: 0.5973, Train Acc: 0.7230, Test Acc: 0.7333, Precision: 0.5706, Recall: 0.7490\n",
      "Epoch: 127, Train Loss: 0.6252, Test Loss: 0.5977, Train Acc: 0.7229, Test Acc: 0.7368, Precision: 0.5710, Recall: 0.7497\n",
      "Epoch: 128, Train Loss: 0.6296, Test Loss: 0.5966, Train Acc: 0.7229, Test Acc: 0.7421, Precision: 0.5722, Recall: 0.7513\n",
      "Epoch: 129, Train Loss: 0.6250, Test Loss: 0.5955, Train Acc: 0.7233, Test Acc: 0.7305, Precision: 0.5702, Recall: 0.7515\n",
      "Epoch: 130, Train Loss: 0.6281, Test Loss: 0.5949, Train Acc: 0.7236, Test Acc: 0.7344, Precision: 0.5716, Recall: 0.7490\n",
      "Epoch: 131, Train Loss: 0.6245, Test Loss: 0.5937, Train Acc: 0.7236, Test Acc: 0.7331, Precision: 0.5712, Recall: 0.7508\n",
      "Epoch: 132, Train Loss: 0.6227, Test Loss: 0.5948, Train Acc: 0.7231, Test Acc: 0.7341, Precision: 0.5705, Recall: 0.7508\n",
      "Epoch: 133, Train Loss: 0.6230, Test Loss: 0.5932, Train Acc: 0.7248, Test Acc: 0.7368, Precision: 0.5710, Recall: 0.7511\n",
      "Epoch: 134, Train Loss: 0.6248, Test Loss: 0.5921, Train Acc: 0.7241, Test Acc: 0.7415, Precision: 0.5720, Recall: 0.7522\n",
      "Epoch: 135, Train Loss: 0.6237, Test Loss: 0.5922, Train Acc: 0.7243, Test Acc: 0.7407, Precision: 0.5728, Recall: 0.7540\n",
      "Epoch: 136, Train Loss: 0.6236, Test Loss: 0.5905, Train Acc: 0.7229, Test Acc: 0.7387, Precision: 0.5717, Recall: 0.7522\n",
      "Epoch: 137, Train Loss: 0.6221, Test Loss: 0.5917, Train Acc: 0.7241, Test Acc: 0.7349, Precision: 0.5703, Recall: 0.7516\n",
      "Epoch: 138, Train Loss: 0.6184, Test Loss: 0.5935, Train Acc: 0.7236, Test Acc: 0.7374, Precision: 0.5705, Recall: 0.7520\n",
      "Epoch: 139, Train Loss: 0.6215, Test Loss: 0.5913, Train Acc: 0.7237, Test Acc: 0.7356, Precision: 0.5715, Recall: 0.7521\n",
      "Epoch: 140, Train Loss: 0.6215, Test Loss: 0.5894, Train Acc: 0.7261, Test Acc: 0.7380, Precision: 0.5719, Recall: 0.7522\n",
      "Epoch: 141, Train Loss: 0.6193, Test Loss: 0.5897, Train Acc: 0.7258, Test Acc: 0.7389, Precision: 0.5730, Recall: 0.7542\n",
      "Epoch: 142, Train Loss: 0.6163, Test Loss: 0.5900, Train Acc: 0.7261, Test Acc: 0.7361, Precision: 0.5717, Recall: 0.7521\n",
      "Epoch: 143, Train Loss: 0.6173, Test Loss: 0.5878, Train Acc: 0.7268, Test Acc: 0.7389, Precision: 0.5732, Recall: 0.7553\n",
      "Epoch: 144, Train Loss: 0.6184, Test Loss: 0.5887, Train Acc: 0.7262, Test Acc: 0.7379, Precision: 0.5721, Recall: 0.7532\n",
      "Epoch: 145, Train Loss: 0.6197, Test Loss: 0.5884, Train Acc: 0.7253, Test Acc: 0.7361, Precision: 0.5706, Recall: 0.7529\n",
      "Epoch: 146, Train Loss: 0.6181, Test Loss: 0.5877, Train Acc: 0.7269, Test Acc: 0.7385, Precision: 0.5723, Recall: 0.7545\n",
      "Epoch: 147, Train Loss: 0.6153, Test Loss: 0.5876, Train Acc: 0.7264, Test Acc: 0.7354, Precision: 0.5728, Recall: 0.7561\n",
      "Epoch: 148, Train Loss: 0.6189, Test Loss: 0.5865, Train Acc: 0.7262, Test Acc: 0.7365, Precision: 0.5716, Recall: 0.7548\n",
      "Epoch: 149, Train Loss: 0.6173, Test Loss: 0.5863, Train Acc: 0.7269, Test Acc: 0.7397, Precision: 0.5723, Recall: 0.7551\n",
      "Epoch: 150, Train Loss: 0.6168, Test Loss: 0.5881, Train Acc: 0.7257, Test Acc: 0.7368, Precision: 0.5726, Recall: 0.7540\n",
      "Epoch: 151, Train Loss: 0.6143, Test Loss: 0.5861, Train Acc: 0.7273, Test Acc: 0.7367, Precision: 0.5720, Recall: 0.7552\n",
      "Epoch: 152, Train Loss: 0.6128, Test Loss: 0.5866, Train Acc: 0.7277, Test Acc: 0.7375, Precision: 0.5721, Recall: 0.7559\n",
      "Epoch: 153, Train Loss: 0.6110, Test Loss: 0.5851, Train Acc: 0.7286, Test Acc: 0.7406, Precision: 0.5722, Recall: 0.7555\n",
      "Epoch: 154, Train Loss: 0.6141, Test Loss: 0.5855, Train Acc: 0.7283, Test Acc: 0.7364, Precision: 0.5722, Recall: 0.7541\n",
      "Epoch: 155, Train Loss: 0.6115, Test Loss: 0.5847, Train Acc: 0.7284, Test Acc: 0.7367, Precision: 0.5716, Recall: 0.7550\n",
      "Epoch: 156, Train Loss: 0.6103, Test Loss: 0.5850, Train Acc: 0.7275, Test Acc: 0.7424, Precision: 0.5722, Recall: 0.7559\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Training and testing loop\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1001\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     train()\n\u001b[1;32m    102\u001b[0m     test_acc, test_loss, precision, recall \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_accs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes  \u001b[38;5;66;03m# Accumulate loss for this epoch.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Derive gradients.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update parameters based on gradients.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "num_classes = 3\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = GCN(hidden_channels=64, num_classes=num_classes).to(device)  # Make sure GCN uses correct number of classes\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Calculate class weights inversely proportional to class frequencies\n",
    "class_weights = []\n",
    "for class_idx in range(num_classes):\n",
    "    # Get counts for this class\n",
    "    class_count = train_class_counts[class_idx]\n",
    "    # Calculate weight (inversely proportional to frequency)\n",
    "    if class_count > 0:\n",
    "        weight = train_total_nodes / (num_classes * class_count)\n",
    "    else:\n",
    "        weight = 1.0\n",
    "    class_weights.append(weight)\n",
    "\n",
    "# Normalize weights\n",
    "class_weights = [w / sum(class_weights) * num_classes for w in class_weights]\n",
    "class_weights_tensor = torch.tensor(class_weights, device=device)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Use weighted cross entropy loss\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "losses = []  # Training losses\n",
    "test_losses = []  # Test losses\n",
    "accs = []  # Test accuracies\n",
    "precisions = []  # Test precisions\n",
    "recalls = []  # Test recalls\n",
    "train_accs = []  # Training accuracies\n",
    "\n",
    "# Define the training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)  # Move batch data to GPU\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        total_loss += loss.item() * data.num_nodes  # Accumulate loss for this epoch.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        \n",
    "        # Calculate accuracy for node predictions\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct = int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        total_correct += correct\n",
    "        total_nodes += data.num_nodes\n",
    "    \n",
    "    avg_loss = total_loss / total_nodes  # Calculate average loss per node\n",
    "    acc = total_correct / total_nodes  # Calculate accuracy per node\n",
    "    train_accs.append(acc)  # Append accuracy to the list.\n",
    "    losses.append(avg_loss)  # Append the average loss to the list.\n",
    "\n",
    "# Define the testing loop\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    total_nodes = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation.\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)  # Move batch data to GPU\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out, data.y)  # Compute the loss for the batch.\n",
    "            total_loss += loss.item() * data.num_nodes  # Accumulate test loss.\n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct = int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            total_correct += correct\n",
    "            total_nodes += data.num_nodes\n",
    "\n",
    "            # Collect predictions and true labels for precision and recall\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(data.y.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total_nodes  # Calculate average loss per node\n",
    "    test_losses.append(avg_loss)  # Append the average test loss to the list.\n",
    "    acc = total_correct / total_nodes  # Calculate accuracy per node\n",
    "    accs.append(acc)  # Append accuracy to the list.\n",
    "\n",
    "    # Compute precision and recall\n",
    "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "    return acc, avg_loss, precision, recall\n",
    "\n",
    "# Training and testing loop\n",
    "for epoch in range(1, 1001):\n",
    "    train()\n",
    "    test_acc, test_loss, precision, recall = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {losses[-1]:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_accs[-1]:.4f}, Test Acc: {test_acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f2821-6520-405a-b862-d9d8ea644927",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1110016-0917-4e8d-9c07-d6d5f56fde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(range(1, 1001), losses, label='Training Loss')\n",
    "plt.plot(range(1,1001), test_losses, label='Test Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.savefig('../Data/loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc70de-0dfe-4bc5-b5c1-bfaafba4e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 1001), accs, label='Test accuracy')\n",
    "# plt.plot(range(1, 101), train_accs, label='Train accuracy')\n",
    "print(f'Max accuracy: {max(accs)}')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves')\n",
    "# plt.savefig('../Data/accuracy_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea127ab-fef7-4ad4-ab82-caa9f996774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 1001), precisions, label='Precision')\n",
    "plt.plot(range(1, 1001), recalls, label='Recall')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.title('Precision and Recall Curves')\n",
    "plt.savefig('../Data/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
